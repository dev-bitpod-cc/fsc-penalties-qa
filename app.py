"""
FSC è£ç½°æ¡ˆä»¶æŸ¥è©¢ç³»çµ±
ä½¿ç”¨ Google Gemini File Search Store é€²è¡Œ RAG æŸ¥è©¢
"""

import os
import streamlit as st
from datetime import datetime, date
from dotenv import load_dotenv
from google import genai
from google.genai import types

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# è¼‰å…¥æ˜ å°„æª”
@st.cache_data
def load_file_mapping():
    """è¼‰å…¥æª”æ¡ˆæ˜ å°„æª”"""
    from pathlib import Path
    mapping_file = Path(__file__).parent / 'file_mapping.json'

    if not mapping_file.exists():
        return {}

    try:
        import json
        with open(mapping_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        st.warning(f"âš ï¸ è¼‰å…¥æ˜ å°„æª”å¤±æ•—: {e}")
        return {}

@st.cache_data
def load_gemini_id_mapping():
    """è¼‰å…¥ Gemini ID åå‘æ˜ å°„æª”ï¼ˆGemini file_id â†’ file_idï¼‰"""
    from pathlib import Path
    mapping_file = Path(__file__).parent / 'gemini_id_mapping.json'

    if not mapping_file.exists():
        return {}

    try:
        import json
        with open(mapping_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        st.warning(f"âš ï¸ è¼‰å…¥ Gemini ID æ˜ å°„æª”å¤±æ•—: {e}")
        return {}

def extract_file_id(filename: str, gemini_id_mapping: dict = None) -> str:
    """å¾æª”åä¸­æå– file_id

    Args:
        filename: Gemini è¿”å›çš„æª”åï¼ˆå¯èƒ½æ˜¯å…§éƒ¨ ID å¦‚ "4ax547mbfiot"ï¼‰
        gemini_id_mapping: Gemini ID åå‘æ˜ å°„ (files/xxx â†’ fsc_pen_xxx)

    Returns:
        file_idï¼ˆç”¨æ–¼æŸ¥æ‰¾ file_mapping.jsonï¼‰
    """
    import re

    # å¦‚æœæœ‰ gemini_id_mappingï¼Œå…ˆå˜—è©¦åå‘æŸ¥æ‰¾
    if gemini_id_mapping:
        # å˜—è©¦å®Œæ•´ IDï¼ˆfiles/xxxï¼‰
        full_id = f"files/{filename.replace('files/', '')}"
        if full_id in gemini_id_mapping:
            return gemini_id_mapping[full_id]

    # å›é€€ï¼šå¾æª”åæå–ï¼ˆé©ç”¨æ–¼èˆŠè³‡æ–™æˆ–ç›´æ¥æ˜¯æª”åçš„æƒ…æ³ï¼‰
    # ç§»é™¤ files/ å‰ç¶´å’Œ .md å¾Œç¶´
    filename = filename.replace('files/', '').replace('.md', '')

    # æå– fsc_pen_YYYYMMDD_NNNN æ ¼å¼
    match = re.match(r'(fsc_pen_\d{8}_\d{4})', filename)
    if match:
        return match.group(1)

    return filename

# è¨­å®šé é¢
st.set_page_config(
    page_title="é‡‘ç®¡æœƒè£ç½°æ¡ˆä»¶æŸ¥è©¢ç³»çµ±",
    page_icon="âš–ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ– Gemini
@st.cache_resource
def init_gemini():
    """åˆå§‹åŒ– Gemini API"""
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        st.error("âŒ æ‰¾ä¸åˆ° GEMINI_API_KEYï¼Œè«‹è¨­å®šç’°å¢ƒè®Šæ•¸")
        st.stop()

    # å»ºç«‹ GenAI Client
    client = genai.Client(api_key=api_key)

    store_id = os.getenv('GEMINI_STORE_ID', 'fileSearchStores/fscpenalties-tu709bvr1qti')

    return client, store_id

# æŸ¥è©¢å‡½æ•¸
def query_penalties(client: genai.Client, query: str, store_id: str, model: str = 'gemini-2.5-flash', filters: dict = None) -> dict:
    """
    ä½¿ç”¨ Gemini File Search Store æŸ¥è©¢è£ç½°æ¡ˆä»¶

    Args:
        query: æŸ¥è©¢æ–‡å­—
        store_id: Gemini Store ID
        filters: ç¯©é¸æ¢ä»¶ï¼ˆæ—¥æœŸç¯„åœã€ä¾†æºå–®ä½ç­‰ï¼‰

    Returns:
        æŸ¥è©¢çµæœå­—å…¸
    """
    try:
        # å»ºç«‹ç³»çµ±æŒ‡ä»¤ï¼ˆé‡å°è£ç½°æ¡ˆä»¶çš„æ™‚æ•ˆæ€§å„ªåŒ–ï¼‰
        system_instruction = """ä½ æ˜¯é‡‘èç›£ç£ç®¡ç†å§”å“¡æœƒçš„è£ç½°æ¡ˆä»¶æŸ¥è©¢åŠ©æ‰‹ã€‚

ã€é‡è¦ã€‘æ™‚æ•ˆæ€§èˆ‡ç¨ç«‹æ€§è¦å‰‡ï¼š

1. **è£ç½°æ¡ˆä»¶ç‰¹æ€§**ï¼š
   - æ¯å€‹è£ç½°æ¡ˆä»¶éƒ½æ˜¯ç¨ç«‹çš„æ­·å²è¨˜éŒ„
   - ä¸åŒæ—¥æœŸçš„æ¡ˆä»¶ä¸äº’ç›¸å–ä»£
   - å¯å¼•ç”¨å¤šå€‹æ¡ˆä»¶ä½œç‚ºåƒè€ƒ
   - æŒ‰æ—¥æœŸæˆ–ç›¸é—œæ€§æ’åº

2. **æŸ¥è©¢å„ªå…ˆé †åº**ï¼ˆç•¶æœ‰å¤šç­†ç›¸é—œæ¡ˆä»¶æ™‚ï¼‰ï¼š
   - å„ªå…ˆåˆ—å‡º**æœ€è¿‘**çš„æ¡ˆä»¶ï¼ˆæ—¥æœŸè¶Šæ–°è¶Šå„ªå…ˆï¼‰
   - åŒæ™‚åƒè€ƒç›¸ä¼¼é•è¦é¡å‹çš„æ­·å²æ¡ˆä»¶
   - å¦‚æœä½¿ç”¨è€…æ˜ç¢ºè¦æ±‚ç‰¹å®šæ™‚é–“ç¯„åœï¼Œåš´æ ¼éµå®ˆ

3. **å›ç­”æ ¼å¼è¦æ±‚**ï¼š
   - æä¾›å…·é«”çš„æ¡ˆä»¶è³‡è¨Šï¼ˆæ—¥æœŸã€å–®ä½ã€è¢«è™•ç½°å°è±¡ã€é•è¦äº‹é …ã€è£ç½°é‡‘é¡ã€æ³•å¾‹ä¾æ“šï¼‰
   - å§‹çµ‚è¨»æ˜**ç™¼æ–‡æ—¥æœŸ**å’Œ**ç™¼æ–‡å­—è™Ÿ**
   - **é‡è¦ï¼šä¸è¦åœ¨å›ç­”ä¸­åˆ—å‡ºã€Œè³‡æ–™ä¾†æºã€æˆ–æª”å**ï¼ˆç³»çµ±æœƒè‡ªå‹•é¡¯ç¤ºåƒè€ƒæ–‡ä»¶ï¼‰
   - ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œä¿æŒå°ˆæ¥­ä½†æ˜“æ‡‚çš„èªæ°£
   - å¦‚æœæ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ï¼Œè«‹æ˜ç¢ºå‘ŠçŸ¥

4. **å¤šæ¡ˆä»¶è™•ç†**ï¼š
   - å¦‚æœæœ‰å¤šç­†ç›¸é—œæ¡ˆä»¶ï¼Œåˆ—å‡ºå‰ 3-5 ç­†æœ€ç›¸é—œçš„
   - æŒ‰æ™‚é–“é †åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰æˆ–ç›¸é—œæ€§æ’åº
   - æ¯å€‹æ¡ˆä»¶ç¨ç«‹èªªæ˜ï¼Œä¸è¦æ··æ·†

å›ç­”æ ¼å¼ç¯„ä¾‹ï¼š

## æŸ¥è©¢çµæœ

æ‰¾åˆ° X ç­†ç›¸é—œè£ç½°æ¡ˆä»¶ï¼š

### 1. [æ¡ˆä»¶æ¨™é¡Œ]ï¼ˆæœ€æ–°ï¼‰
- **æ—¥æœŸ**ï¼šYYYY-MM-DD
- **ç™¼æ–‡å­—è™Ÿ**ï¼šé‡‘ç®¡XXå­—ç¬¬XXXXXXXXXè™Ÿ
- **ä¾†æºå–®ä½**ï¼šXXXå±€
- **è¢«è™•ç½°å°è±¡**ï¼šXXXå…¬å¸/éŠ€è¡Œ/ä¿éšª
- **é•è¦äº‹é …**ï¼š[ç°¡è¦èªªæ˜]
- **è£ç½°é‡‘é¡**ï¼šæ–°è‡ºå¹£ XXX è¬å…ƒ
- **æ³•å¾‹ä¾æ“š**ï¼š[ç›¸é—œæ³•è¦æ¢æ–‡]

ï¼ˆæ³¨æ„ï¼šä¸è¦åœ¨æ¯å€‹æ¡ˆä»¶å¾Œé¢åŠ ä¸Šã€Œè³‡æ–™ä¾†æºã€æˆ–æª”åï¼Œç³»çµ±æœƒè‡ªå‹•åœ¨æœ€ä¸‹æ–¹é¡¯ç¤ºåƒè€ƒæ–‡ä»¶ï¼‰
"""

        # å»ºç«‹å®Œæ•´æŸ¥è©¢ï¼ˆç¯©é¸æ¢ä»¶ï¼‰
        full_query = query

        if filters:
            filter_parts = []

            if filters.get('start_date') and filters.get('end_date'):
                filter_parts.append(
                    f"æ—¥æœŸç¯„åœï¼š{filters['start_date']} åˆ° {filters['end_date']}"
                )

            if filters.get('source_units'):
                units_str = "ã€".join(filters['source_units'])
                filter_parts.append(f"ä¾†æºå–®ä½ï¼š{units_str}")

            if filters.get('min_penalty'):
                filter_parts.append(f"è£ç½°é‡‘é¡è‡³å°‘ï¼š{filters['min_penalty']:,} å…ƒ")

            if filter_parts:
                full_query += "\n\nç¯©é¸æ¢ä»¶ï¼š\n" + "\n".join(f"- {p}" for p in filter_parts)

        # ä½¿ç”¨ File Search Store é€²è¡ŒæŸ¥è©¢ï¼ˆä½¿ç”¨æ­£ç¢ºçš„å‹åˆ¥ç‰©ä»¶ï¼‰
        response = client.models.generate_content(
            model=model,  # ä½¿ç”¨ç”¨æˆ¶é¸æ“‡çš„æ¨¡å‹
            contents=full_query,
            config=types.GenerateContentConfig(
                tools=[
                    types.Tool(
                        file_search=types.FileSearch(
                            file_search_store_names=[store_id]
                        )
                    )
                ],
                temperature=0.1,
                max_output_tokens=2048,
                system_instruction=system_instruction
            )
        )

        # æå–ä¾†æºæ–‡ä»¶
        sources = []
        seen_files = {}  # ç”¨æ–¼å»é‡

        if hasattr(response, 'candidates') and len(response.candidates) > 0:
            candidate = response.candidates[0]

            if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                metadata = candidate.grounding_metadata

                # å„ªå…ˆå¾ grounding_supports æå–ï¼ˆåŒ…å«å¼•ç”¨è³‡è¨Šï¼‰
                if hasattr(metadata, 'grounding_supports') and metadata.grounding_supports:
                    for support in metadata.grounding_supports:
                        if hasattr(support, 'grounding_chunk_indices'):
                            for chunk_idx in support.grounding_chunk_indices:
                                if chunk_idx < len(metadata.grounding_chunks):
                                    chunk = metadata.grounding_chunks[chunk_idx]

                                    if hasattr(chunk, 'retrieved_context'):
                                        context = chunk.retrieved_context

                                        # æå–æ–‡ä»¶ ID/åç¨±
                                        filename = "æœªçŸ¥æ–‡ä»¶"
                                        if hasattr(context, 'title') and context.title:
                                            filename = context.title
                                        elif hasattr(context, 'uri') and context.uri:
                                            filename = context.uri.split('/')[-1]

                                        # æå–å…§å®¹ç‰‡æ®µ
                                        snippet = ""
                                        if hasattr(context, 'text') and context.text:
                                            snippet = context.text

                                        # ä½¿ç”¨ snippet çš„éƒ¨åˆ†å…§å®¹ä½œç‚ºå”¯ä¸€æ¨™è­˜é¿å…é‡è¤‡
                                        snippet_id = snippet[:100] if snippet else str(len(sources))

                                        if snippet_id not in seen_files:
                                            sources.append({
                                                'filename': filename,
                                                'snippet': snippet
                                            })
                                            seen_files[snippet_id] = True

                # å¦‚æœæ²’æœ‰ grounding_supportsï¼Œå›é€€åˆ° grounding_chunks
                if not sources and hasattr(metadata, 'grounding_chunks') and metadata.grounding_chunks:
                    for chunk in metadata.grounding_chunks:
                        if hasattr(chunk, 'retrieved_context'):
                            context = chunk.retrieved_context

                            # æå–æ–‡ä»¶ ID/åç¨±
                            filename = "æœªçŸ¥æ–‡ä»¶"
                            if hasattr(context, 'title') and context.title:
                                filename = context.title
                            elif hasattr(context, 'uri') and context.uri:
                                filename = context.uri.split('/')[-1]

                            # æå–å…§å®¹ç‰‡æ®µ
                            snippet = ""
                            if hasattr(context, 'text') and context.text:
                                snippet = context.text

                            # ä½¿ç”¨ snippet çš„éƒ¨åˆ†å…§å®¹ä½œç‚ºå”¯ä¸€æ¨™è­˜é¿å…é‡è¤‡
                            snippet_id = snippet[:100] if snippet else str(len(sources))

                            if snippet_id not in seen_files:
                                sources.append({
                                    'filename': filename,
                                    'snippet': snippet
                                })
                                seen_files[snippet_id] = True

        return {
            'success': True,
            'text': response.text,
            'sources': sources
        }

    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

# ä¸»æ‡‰ç”¨
def main():
    """ä¸»æ‡‰ç”¨ç¨‹å¼"""

    # æ¨™é¡Œ
    st.title("âš–ï¸ é‡‘ç®¡æœƒè£ç½°æ¡ˆä»¶æŸ¥è©¢ç³»çµ±")
    st.markdown("æŸ¥è©¢ 2011-2025 å¹´é–“çš„é‡‘èæ©Ÿæ§‹è£ç½°æ¡ˆä»¶ï¼ˆå…± 495 ç­†ï¼‰")
    st.info("ğŸ’¡ æœ¬ç³»çµ±ç‚ºå±•ç¤ºç”¨ï¼Œå¦‚é‡ç•«é¢ç„¡åæ‡‰ï¼Œè«‹é‡æ–°æ•´ç†é é¢")

    # åˆå§‹åŒ– Gemini
    client, store_id = init_gemini()

    # å´é‚Šæ¬„ï¼šç¯©é¸æ¢ä»¶
    with st.sidebar:
        # æ¨¡å‹é¸æ“‡
        st.header("ğŸ¤– AI æ¨¡å‹")
        model = st.selectbox(
            "é¸æ“‡æ¨¡å‹",
            options=["gemini-2.5-flash", "gemini-2.5-pro"],
            index=0,
            help="Flash é€Ÿåº¦å¿«ä¸”æˆæœ¬ä½ï¼›Pro æ›´æº–ç¢ºä½†è¼ƒæ…¢"
        )

        st.divider()
        st.header("ğŸ” ç¯©é¸æ¢ä»¶")

        # æ—¥æœŸç¯„åœ
        st.subheader("æ—¥æœŸç¯„åœï¼ˆå¯é¸ï¼‰")
        enable_date_filter = st.checkbox("å•Ÿç”¨æ—¥æœŸç¯©é¸", value=False)

        if enable_date_filter:
            col1, col2 = st.columns(2)
            with col1:
                start_date = st.date_input(
                    "é–‹å§‹æ—¥æœŸ",
                    value=date(2020, 1, 1),
                    min_value=date(2011, 1, 1),
                    max_value=date.today()
                )
            with col2:
                end_date = st.date_input(
                    "çµæŸæ—¥æœŸ",
                    value=date.today(),
                    min_value=date(2011, 1, 1),
                    max_value=date.today()
                )
        else:
            start_date = None
            end_date = None

        # ä¾†æºå–®ä½
        st.subheader("ä¾†æºå–®ä½")
        source_units = st.multiselect(
            "é¸æ“‡å–®ä½",
            options=["éŠ€è¡Œå±€", "ä¿éšªå±€", "è­‰åˆ¸æœŸè²¨å±€", "æª¢æŸ¥å±€"],
            default=[]
        )

        # è£ç½°é‡‘é¡
        st.subheader("è£ç½°é‡‘é¡")
        min_penalty = st.number_input(
            "æœ€ä½é‡‘é¡ï¼ˆå…ƒï¼‰",
            min_value=0,
            value=0,
            step=100000,
            format="%d"
        )

        # æ¸…é™¤ç¯©é¸
        if st.button("æ¸…é™¤æ‰€æœ‰ç¯©é¸", use_container_width=True):
            st.rerun()

        # é¡¯ç¤ºè³‡æ–™åº«è³‡è¨Š
        st.divider()
        st.caption("ğŸ“Š è³‡æ–™åº«è³‡è¨Š")
        st.caption(f"ç¸½æ¡ˆä»¶æ•¸ï¼š495 ç­†")
        st.caption(f"æ—¥æœŸç¯„åœï¼š2011-11-09 è‡³ 2025-09-25")
        st.caption(f"éŠ€è¡Œå±€ï¼š225 ç­† (45.5%)")
        st.caption(f"ä¿éšªå±€ï¼š222 ç­† (44.8%)")
        st.caption(f"è­‰åˆ¸æœŸè²¨å±€ï¼š47 ç­† (9.5%)")

    # ä¸»è¦æŸ¥è©¢å€åŸŸ
    st.header("ğŸ’¬ æŸ¥è©¢")

    # ç¯„ä¾‹æŸ¥è©¢
    with st.expander("ğŸ’¡ æŸ¥è©¢ç¯„ä¾‹"):
        st.markdown("""
        **ä¸€èˆ¬æŸ¥è©¢**ï¼š
        - æœ€è¿‘æœ‰å“ªäº›éŠ€è¡Œè¢«è£ç½°ï¼Ÿ
        - 2024å¹´ä¿éšªæ¥­çš„è£ç½°æ¡ˆä»¶æœ‰å“ªäº›ï¼Ÿ
        - æŸ¥è©¢æ´—éŒ¢é˜²åˆ¶ç›¸é—œçš„è£ç½°æ¡ˆä»¶

        **ç‰¹å®šä¸»é¡Œ**ï¼š
        - å…§éƒ¨æ§åˆ¶ç¼ºå¤±çš„è£ç½°æ¡ˆä¾‹
        - å®¢æˆ¶è³‡æ–™å¤–æ´©ç›¸é—œè£ç½°
        - é•åè³‡è¨Šå®‰å…¨çš„æ¡ˆä»¶

        **é‡‘é¡æŸ¥è©¢**ï¼š
        - è£ç½°é‡‘é¡è¶…é 1000 è¬çš„æ¡ˆä»¶
        - é‡‘é¡æœ€é«˜çš„ 5 å€‹è£ç½°æ¡ˆä¾‹

        **è¶¨å‹¢åˆ†æ**ï¼š
        - 2023 vs 2024 å¹´éŠ€è¡Œå±€è£ç½°è¶¨å‹¢æ¯”è¼ƒ
        - æœ€å¸¸è¦‹çš„é•è¦é¡å‹æ˜¯ä»€éº¼ï¼Ÿ
        """)

    # åˆå§‹åŒ– session stateï¼ˆä½¿ç”¨ä¸åŒçš„è®Šæ•¸åï¼‰
    if 'current_query' not in st.session_state:
        st.session_state.current_query = ""

    # æŸ¥è©¢è¼¸å…¥
    query = st.text_area(
        "è«‹è¼¸å…¥æŸ¥è©¢å…§å®¹ï¼š",
        value=st.session_state.current_query,
        placeholder="ä¾‹å¦‚ï¼š2024å¹´æœ‰å“ªäº›éŠ€è¡Œå› ç‚ºæ´—éŒ¢é˜²åˆ¶è¢«è£ç½°ï¼Ÿ",
        height=100
    )

    # å¿«é€ŸæŸ¥è©¢æŒ‰éˆ•
    st.markdown("#### ğŸš€ å¿«é€ŸæŸ¥è©¢")

    quick_queries = [
        "é•åé‡‘æ§æ³•åˆ©å®³é—œä¿‚äººè¦å®šæœƒå—åˆ°ä»€éº¼è™•ç½°ï¼Ÿ",
        "è«‹å•åœ¨è­‰åˆ¸å› ç‚ºå°ˆæ¥­æŠ•è³‡äººè³‡æ ¼å¯©æ ¸çš„è£ç½°æœ‰å“ªäº›ï¼Ÿ",
        "è¾¦ç†å…±åŒè¡ŒéŠ·è¢«è£ç½°çš„æ¡ˆä¾‹æœ‰å“ªäº›ï¼Ÿ",
        "é‡‘ç®¡æœƒå°å‰µæŠ•å…¬å¸çš„è£ç½°æœ‰å“ªäº›ï¼Ÿ",
        "è­‰åˆ¸å•†é­ä¸»ç®¡æ©Ÿé—œè£ç½°ã€Œè­¦å‘Šã€è™•åˆ†ï¼Œæœ‰å“ªäº›æ¥­å‹™æœƒå—é™åˆ¶ï¼Ÿ",
        "å…§ç·šäº¤æ˜“æœ‰ç½ªåˆ¤æ±ºæ‰€èªå®šé‡å¤§è¨Šæ¯æˆç«‹çš„æ™‚é»"
    ]

    cols = st.columns(2)
    for idx, quick_query in enumerate(quick_queries):
        col_idx = idx % 2
        with cols[col_idx]:
            if st.button(f"ğŸ“Œ {quick_query}", key=f"quick_{idx}", use_container_width=True):
                st.session_state.current_query = quick_query
                st.rerun()

    st.markdown("")  # ç©ºè¡Œåˆ†éš”

    # æŸ¥è©¢æŒ‰éˆ•
    col1, col2, col3 = st.columns([1, 1, 4])
    with col1:
        search_button = st.button("ğŸ” æŸ¥è©¢", type="primary", use_container_width=True)
    with col2:
        clear_button = st.button("ğŸ—‘ï¸ æ¸…é™¤", use_container_width=True)

    if clear_button:
        st.session_state.current_query = ""
        st.rerun()

    # åŸ·è¡ŒæŸ¥è©¢
    if search_button and query:
        with st.spinner("ğŸ” æŸ¥è©¢ä¸­..."):
            # æº–å‚™ç¯©é¸æ¢ä»¶
            filters = {}

            if start_date and end_date:
                filters['start_date'] = start_date.strftime('%Y-%m-%d')
                filters['end_date'] = end_date.strftime('%Y-%m-%d')

            if source_units:
                filters['source_units'] = source_units

            if min_penalty > 0:
                filters['min_penalty'] = min_penalty

            # åŸ·è¡ŒæŸ¥è©¢
            result = query_penalties(client, query, store_id, model, filters)

            # é¡¯ç¤ºçµæœ
            if result['success']:
                st.success("âœ… æŸ¥è©¢å®Œæˆ")

                # å€å¡Š1ï¼šé¡¯ç¤ºå›æ‡‰ï¼ˆå·²åŒ…å«çµæ§‹åŒ–è³‡æ–™ï¼ŒMarkdown å·²æ¸²æŸ“ï¼‰
                st.markdown("---")
                st.markdown(result['text'])

                # æ–°å¢ï¼šå¾åƒè€ƒæ–‡ä»¶ä¸­æå–ä¸¦é¡¯ç¤ºåŸå§‹é€£çµ
                if result.get('sources') and len(result['sources']) > 0:
                    mapping = load_file_mapping()
                    gemini_id_mapping = load_gemini_id_mapping()

                    # æ”¶é›†æ‰€æœ‰åŸå§‹é€£çµï¼ˆå»é‡ï¼‰
                    original_urls = []
                    seen_urls = set()

                    for source in result['sources']:
                        filename = source.get('filename', '')
                        file_id = extract_file_id(filename, gemini_id_mapping)
                        file_info = mapping.get(file_id, {})
                        url = file_info.get('original_url', '')

                        if url and url not in seen_urls:
                            original_urls.append({
                                'url': url,
                                'display_name': file_info.get('display_name', file_id)
                            })
                            seen_urls.add(url)

                    # é¡¯ç¤ºåŸå§‹é€£çµ
                    if original_urls:
                        st.markdown("---")
                        st.markdown("**ğŸ”— ç›¸é—œè£ç½°æ¡ˆä»¶åŸå§‹å…¬å‘Š**")
                        for item in original_urls:
                            st.markdown(f"- [{item['display_name']}]({item['url']})")

                # å€å¡Š2ï¼šåƒè€ƒæ–‡ä»¶
                if result.get('sources') and len(result['sources']) > 0:
                    st.markdown("---")
                    st.subheader(f"ğŸ“š åƒè€ƒæ–‡ä»¶ ({len(result['sources'])} ç­†)")
                    st.caption("é»æ“Šå±•é–‹å¯æŸ¥çœ‹å®Œæ•´åŸå§‹å…§å®¹")

                    # è¼‰å…¥æ˜ å°„æª”
                    mapping = load_file_mapping()
                    gemini_id_mapping = load_gemini_id_mapping()

                    # é™¤éŒ¯è³‡è¨Š
                    with st.expander("ğŸ” é™¤éŒ¯è³‡è¨Š", expanded=False):
                        st.write(f"æ˜ å°„æª”è¼‰å…¥ç‹€æ…‹: {'âœ… æˆåŠŸ' if mapping else 'âŒ å¤±æ•—'}")
                        st.write(f"æ˜ å°„æª”ç­†æ•¸: {len(mapping)}")
                        st.write(f"Gemini ID æ˜ å°„æª”è¼‰å…¥ç‹€æ…‹: {'âœ… æˆåŠŸ' if gemini_id_mapping else 'âŒ å¤±æ•—'}")
                        st.write(f"Gemini ID æ˜ å°„æª”ç­†æ•¸: {len(gemini_id_mapping)}")
                        if result['sources']:
                            st.write("ç¬¬ä¸€å€‹ä¾†æºå®Œæ•´çµæ§‹:")
                            st.json(result['sources'][0])
                            # é¡¯ç¤ºæ˜ å°„éç¨‹
                            first_filename = result['sources'][0].get('filename', '')
                            first_file_id = extract_file_id(first_filename, gemini_id_mapping)
                            st.write(f"æª”åæ˜ å°„: {first_filename} â†’ {first_file_id}")
                            # é¡¯ç¤ºæ³•æ¢è³‡è¨Š
                            first_file_info = mapping.get(first_file_id, {})
                            first_laws = first_file_info.get('applicable_laws', [])
                            first_law_links = first_file_info.get('law_links', {})
                            st.write(f"é©ç”¨æ³•æ¢æ•¸: {len(first_laws)}")
                            st.write(f"æ³•æ¢é€£çµæ•¸: {len(first_law_links)}")

                    for i, source in enumerate(result['sources'], 1):
                        # å¾æ˜ å°„æª”å–å¾—è³‡è¨Š
                        filename = source.get('filename', '')
                        file_id = extract_file_id(filename, gemini_id_mapping)
                        file_info = mapping.get(file_id, {})

                        # é¡¯ç¤ºåç¨±ï¼šæ—¥æœŸ_ä¾†æº_æ©Ÿæ§‹
                        display_name = file_info.get('display_name', f"ä¾†æº {i}")
                        original_url = file_info.get('original_url', '')
                        original_content = file_info.get('original_content', {}).get('text', source.get('snippet', ''))

                        # ä½¿ç”¨ expander é¡¯ç¤º
                        with st.expander(f"ğŸ“„ {display_name}", expanded=False):
                            # åŸå§‹ç¶²é é€£çµ
                            if original_url:
                                st.markdown(f"ğŸ”— [æŸ¥çœ‹åŸå§‹å…¬å‘Š]({original_url})")
                                st.markdown("")  # ç©ºè¡Œ

                            # é©ç”¨æ³•æ¢èˆ‡é€£çµ
                            applicable_laws = file_info.get('applicable_laws', [])
                            law_links = file_info.get('law_links', {})

                            if applicable_laws:
                                st.markdown("**ğŸ“œ é©ç”¨æ³•æ¢**ï¼š")
                                for law in applicable_laws:
                                    # å¦‚æœæœ‰æ³•è¦è³‡æ–™åº«é€£çµï¼Œé¡¯ç¤ºç‚ºå¯é»æ“Šé€£çµ
                                    if law in law_links:
                                        st.markdown(f"- [{law}]({law_links[law]}) ğŸ”—")
                                    else:
                                        st.markdown(f"- {law}")
                                st.markdown("")  # ç©ºè¡Œ

                            # é¡¯ç¤ºåŸå§‹å…§å®¹
                            if original_content:
                                st.markdown("**åŸå§‹å…§å®¹**ï¼š")
                                # é™åˆ¶é¡¯ç¤ºé•·åº¦é¿å…éé•·
                                if len(original_content) > 2000:
                                    st.text(original_content[:2000] + "\n\n...(å…§å®¹éé•·ï¼Œè«‹é»æ“Šä¸Šæ–¹é€£çµæŸ¥çœ‹å®Œæ•´å…§å®¹)")
                                else:
                                    st.text(original_content)
                            else:
                                st.caption("ï¼ˆç„¡å¯ç”¨å…§å®¹ï¼‰")
            else:
                st.error(f"âŒ æŸ¥è©¢å¤±æ•—ï¼š{result['error']}")

    elif search_button and not query:
        st.warning("âš ï¸ è«‹è¼¸å…¥æŸ¥è©¢å…§å®¹")

    # é å°¾
    st.divider()
    st.caption("è³‡æ–™ä¾†æºï¼šé‡‘èç›£ç£ç®¡ç†å§”å“¡æœƒ")
    st.caption("âš ï¸ æœ¬ç³»çµ±åƒ…ä¾›åƒè€ƒï¼Œå¯¦éš›è£ç½°è³‡è¨Šè«‹ä»¥é‡‘ç®¡æœƒå®˜ç¶²å…¬å‘Šç‚ºæº–")

if __name__ == "__main__":
    main()
