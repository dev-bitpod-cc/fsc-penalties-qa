"""
FSC è£ç½°æ¡ˆä»¶æŸ¥è©¢ç³»çµ±
ä½¿ç”¨ Google Gemini File Search Store é€²è¡Œ RAG æŸ¥è©¢
"""

import os
import streamlit as st
from datetime import datetime, date
from dotenv import load_dotenv
from google import genai
from google.genai import types

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# è¼‰å…¥æ˜ å°„æª”
@st.cache_data
def load_file_mapping():
    """è¼‰å…¥æª”æ¡ˆæ˜ å°„æª”"""
    from pathlib import Path
    mapping_file = Path(__file__).parent / 'file_mapping.json'

    if not mapping_file.exists():
        return {}

    try:
        import json
        with open(mapping_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        st.warning(f"âš ï¸ è¼‰å…¥æ˜ å°„æª”å¤±æ•—: {e}")
        return {}

@st.cache_data
def load_gemini_id_mapping():
    """è¼‰å…¥ Gemini ID åå‘æ˜ å°„æª”ï¼ˆGemini file_id â†’ file_idï¼‰"""
    from pathlib import Path
    mapping_file = Path(__file__).parent / 'gemini_id_mapping.json'

    if not mapping_file.exists():
        return {}

    try:
        import json
        with open(mapping_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        st.warning(f"âš ï¸ è¼‰å…¥ Gemini ID æ˜ å°„æª”å¤±æ•—: {e}")
        return {}

def extract_file_id(filename: str, gemini_id_mapping: dict = None) -> str:
    """å¾æª”åä¸­æå– file_id

    Args:
        filename: Gemini è¿”å›çš„æª”åï¼ˆå¯èƒ½æ˜¯å…§éƒ¨ ID å¦‚ "4ax547mbfiot"ï¼‰
        gemini_id_mapping: Gemini ID åå‘æ˜ å°„ (files/xxx â†’ fsc_pen_xxx)

    Returns:
        file_idï¼ˆç”¨æ–¼æŸ¥æ‰¾ file_mapping.jsonï¼‰ï¼Œå¦‚æœæ˜ å°„å¤±æ•—å‰‡è¿”å› None
    """
    import re

    # å¦‚æœæœ‰ gemini_id_mappingï¼Œå…ˆå˜—è©¦åå‘æŸ¥æ‰¾
    if gemini_id_mapping:
        # å˜—è©¦å®Œæ•´ IDï¼ˆfiles/xxxï¼‰
        full_id = f"files/{filename.replace('files/', '')}"
        if full_id in gemini_id_mapping:
            return gemini_id_mapping[full_id]

    # å›é€€ï¼šå¾æª”åæå–ï¼ˆé©ç”¨æ–¼èˆŠè³‡æ–™æˆ–ç›´æ¥æ˜¯æª”åçš„æƒ…æ³ï¼‰
    # ç§»é™¤ files/ å‰ç¶´å’Œ .md å¾Œç¶´
    filename_clean = filename.replace('files/', '').replace('.md', '')

    # æå– fsc_pen_YYYYMMDD_NNNN æ ¼å¼
    match = re.match(r'(fsc_pen_\d{8}_\d{4})', filename_clean)
    if match:
        return match.group(1)

    # å¦‚æœç„¡æ³•æå–æœ‰æ•ˆçš„ file_idï¼Œè¿”å› Noneï¼ˆé¿å…ä½¿ç”¨ç„¡æ•ˆçš„ Gemini å…§éƒ¨ IDï¼‰
    return None

def add_law_links_to_text(text: str, law_links_dict: dict) -> str:
    """åœ¨æ–‡å­—ä¸­ç‚ºæ³•æ¢åŠ å…¥é€£çµ

    Args:
        text: åŸå§‹æ–‡å­—
        law_links_dict: æ³•æ¢åˆ°é€£çµçš„æ˜ å°„ {æ³•æ¢åç¨±: URL}

    Returns:
        åŠ å…¥é€£çµå¾Œçš„æ–‡å­—
    """
    import re

    if not law_links_dict:
        return text

    # æŒ‰æ³•æ¢åç¨±é•·åº¦æ’åºï¼ˆé•·çš„å„ªå…ˆï¼Œé¿å…çŸ­çš„å…ˆè¢«æ›¿æ›å°è‡´é•·çš„ç„¡æ³•åŒ¹é…ï¼‰
    sorted_laws = sorted(law_links_dict.keys(), key=len, reverse=True)

    result = text
    replaced = set()  # è¨˜éŒ„å·²æ›¿æ›çš„æ³•æ¢ï¼Œé¿å…é‡è¤‡æ›¿æ›

    for law in sorted_laws:
        if law in replaced:
            continue

        link = law_links_dict[law]

        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æ‰¾åˆ°æ³•æ¢ï¼ˆç¢ºä¿ä¸åœ¨ Markdown é€£çµä¸­ï¼‰
        # ä¸åŒ¹é…å·²ç¶“æ˜¯é€£çµçš„éƒ¨åˆ†ï¼š[xxx] æˆ– (http...)
        pattern = r'(?<!\[)(?<!\()' + re.escape(law) + r'(?!\])(?!\))'

        # æ›¿æ›ç‚º Markdown é€£çµæ ¼å¼
        replacement = f'[{law}]({link})'

        # åŸ·è¡Œæ›¿æ›
        new_result = re.sub(pattern, replacement, result)

        # å¦‚æœæœ‰æ›¿æ›ç™¼ç”Ÿï¼Œè¨˜éŒ„ä¸‹ä¾†
        if new_result != result:
            replaced.add(law)
            result = new_result

    return result

def insert_case_links_by_order(text: str, case_urls: list) -> str:
    """
    æŒ‰é †åºå°‡æ¡ˆä»¶æ¨™é¡Œè½‰æ›ç‚ºé€£çµï¼ˆå€å¡Š1ç”¨ï¼‰

    Args:
        text: Gemini å›ç­”æ–‡å­—
        case_urls: æ¡ˆä»¶é€£çµåˆ—è¡¨ï¼ˆæŒ‰é †åºï¼Œå¾ grounding_metadata æå–ï¼‰

    Returns:
        æ’å…¥é€£çµå¾Œçš„æ–‡å­—
    """
    import re

    if not case_urls:
        return text

    # æ‰¾å‡ºæ‰€æœ‰æ¨™é¡Œï¼š### 1. [æ¨™é¡Œå…§å®¹]
    pattern = r'(###\s*\d+\.\s+)([^\n]+)'
    matches = list(re.finditer(pattern, text))

    if not matches:
        return text

    # å¾å¾Œå¾€å‰æ›¿æ›ï¼ˆé¿å…ä½ç½®åç§»ï¼‰
    result = text
    for i, match in enumerate(reversed(matches)):
        # åå‘ç´¢å¼•
        idx = len(matches) - 1 - i

        # æª¢æŸ¥æ˜¯å¦æœ‰å°æ‡‰çš„ URL
        if idx >= len(case_urls):
            continue

        prefix = match.group(1)      # "### 1. "
        title = match.group(2).strip()  # "ä¸‰å•†ç¾é‚¦äººå£½ä¿éšªè‚¡ä»½..."
        url = case_urls[idx]

        # æª¢æŸ¥æ˜¯å¦å·²ç¶“æ˜¯é€£çµï¼ˆé¿å…é‡è¤‡æ›¿æ›ï¼‰
        if title.startswith('[') and '](' in title:
            continue

        # æ›¿æ›ç‚ºé€£çµ
        new_text = f"{prefix}[{title}]({url})"
        result = result[:match.start()] + new_text + result[match.end():]

    return result

def remove_social_media_noise(text: str) -> str:
    """
    ç§»é™¤åŸå§‹æ–‡å­—ä¸­çš„ç¤¾ç¾¤åª’é«”åˆ†äº«æŒ‰éˆ•ç­‰é›œè¨Š

    Args:
        text: åŸå§‹æ–‡å­—

    Returns:
        æ¸…ç†å¾Œçš„æ–‡å­—
    """
    import re

    # ç¤¾ç¾¤åª’é«”ç›¸é—œé—œéµå­—
    noise_patterns = [
        r'facebook',
        r'Facebook',
        r'twitter',
        r'Twitter',
        r'line',
        r'LINE',
        r'åˆ†äº«',
        r'åˆ—å°',
        r'è½‰å¯„',
        r'å‹å–„åˆ—å°',
        r'å›ä¸Šä¸€é ',
        r':::',
        r'å›é¦–é ',
        r'ç¶²ç«™å°è¦½',
        r'English',
        r'å…’ç«¥ç‰ˆ',
        r'è¡Œå‹•ç‰ˆ',
        r'RSS',
        r'å­—ç´šå¤§å°',
        r'å° ä¸­ å¤§',
    ]

    # ç§»é™¤åŒ…å«é€™äº›é—œéµå­—çš„è¡Œ
    lines = text.split('\n')
    cleaned_lines = []

    for line in lines:
        line = line.strip()
        # è·³éç©ºè¡Œ
        if not line:
            continue

        # æª¢æŸ¥æ˜¯å¦åŒ…å«é›œè¨Šé—œéµå­—
        is_noise = False
        for pattern in noise_patterns:
            if re.search(pattern, line, re.IGNORECASE):
                is_noise = True
                break

        if not is_noise:
            cleaned_lines.append(line)

    return '\n'.join(cleaned_lines)

def display_grounding_sources_v2(sources: list, file_mapping: dict, gemini_id_mapping: dict, excluded_file_ids: set = None):
    """
    é¡¯ç¤ºä¹Ÿå¯ä»¥å¦å¤–åƒè€ƒï¼ˆå€å¡Š3 - æ–°ç‰ˆï¼‰

    åªé¡¯ç¤ºä¸åœ¨æŸ¥è©¢çµæœæ¨™é¡Œä¸­çš„é¡å¤–åƒè€ƒæ–‡ä»¶

    Args:
        sources: å¾ query_penalties è¿”å›çš„ sources åˆ—è¡¨
        file_mapping: file_mapping.json çš„å…§å®¹
        gemini_id_mapping: Gemini ID æ˜ å°„
        excluded_file_ids: å·²åœ¨æŸ¥è©¢çµæœæ¨™é¡Œä¸­çš„ file_idsï¼ˆè¦æ’é™¤çš„ï¼‰
    """
    if not sources:
        return

    if excluded_file_ids is None:
        excluded_file_ids = set()

    # 1. å»é‡ä¸¦æå– file_idsï¼ˆåªä¿ç•™æœ‰æ•ˆä¸”å­˜åœ¨æ–¼ file_mapping çš„æª”æ¡ˆï¼‰
    unique_file_ids = []
    seen = set()

    for source in sources:
        filename = source.get('filename', '')
        file_id = extract_file_id(filename, gemini_id_mapping)

        # è·³éæ˜ å°„å¤±æ•—æˆ–ä¸å­˜åœ¨æ–¼ file_mapping çš„æª”æ¡ˆ
        if not file_id or file_id not in file_mapping:
            continue

        if file_id not in seen:
            unique_file_ids.append(file_id)
            seen.add(file_id)

    # 2. éæ¿¾æ‰å·²åœ¨æŸ¥è©¢çµæœä¸­çš„æ–‡ä»¶
    additional_file_ids = [fid for fid in unique_file_ids if fid not in excluded_file_ids]

    # 3. å¦‚æœæ²’æœ‰é¡å¤–çš„æ–‡ä»¶ï¼Œä¸é¡¯ç¤ºæ•´å€‹å€å¡Š
    if not additional_file_ids:
        return

    # 4. é¡¯ç¤ºä¹Ÿå¯ä»¥å¦å¤–åƒè€ƒ
    st.subheader(f"ğŸ“š ä¹Ÿå¯ä»¥å¦å¤–åƒè€ƒ ({len(additional_file_ids)} ç­†)")

    for file_id in additional_file_ids:
        # æŸ¥æ‰¾ file_mapping
        file_info = file_mapping.get(file_id, {})
        display_name = file_info.get('display_name', file_id)
        detail_url = file_info.get('original_url', '')
        original_content = file_info.get('original_content', {}).get('text', '')

        # å±•é–‹å¼é¡¯ç¤º
        with st.expander(f"ğŸ“„ {display_name}"):
            # 1. é¡¯ç¤ºåŸå§‹æ¡ˆä»¶é€£çµ
            if detail_url:
                st.markdown(f"ğŸ”— [æŸ¥çœ‹é‡‘ç®¡æœƒåŸå§‹å…¬å‘Š]({detail_url})")
                st.markdown("---")

            # 2. é¡¯ç¤ºåŸå§‹æ¡ˆä»¶ç´”æ–‡å­—å…§å®¹
            st.markdown("**åŸå§‹æ¡ˆä»¶å…§å®¹ï¼š**")

            if original_content:
                # ç§»é™¤ç¤¾ç¾¤åª’é«”é›œè¨Š
                cleaned_content = remove_social_media_noise(original_content)

                # é™åˆ¶é¡¯ç¤ºé•·åº¦ï¼Œå¯æ»¾å‹•
                if len(cleaned_content) > 2000:
                    st.text_area(
                        "",
                        value=cleaned_content[:2000] + "\n\n...(å…§å®¹éé•·ï¼Œè«‹é»æ“Šä¸Šæ–¹é€£çµæŸ¥çœ‹å®Œæ•´å…§å®¹)",
                        height=300,
                        disabled=True,
                        label_visibility="collapsed"
                    )
                else:
                    st.text_area(
                        "",
                        value=cleaned_content,
                        height=300,
                        disabled=True,
                        label_visibility="collapsed"
                    )
            else:
                st.caption("ï¼ˆç„¡å¯ç”¨å…§å®¹ï¼‰")

# è¨­å®šé é¢
st.set_page_config(
    page_title="é‡‘ç®¡æœƒè£ç½°æ¡ˆä»¶æŸ¥è©¢ç³»çµ±",
    page_icon="âš–ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ– Gemini
@st.cache_resource
def init_gemini():
    """åˆå§‹åŒ– Gemini API"""
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        st.error("âŒ æ‰¾ä¸åˆ° GEMINI_API_KEYï¼Œè«‹è¨­å®šç’°å¢ƒè®Šæ•¸")
        st.stop()

    # å»ºç«‹ GenAI Client
    client = genai.Client(api_key=api_key)

    store_id = os.getenv('GEMINI_STORE_ID', 'fileSearchStores/fscpenalties-tu709bvr1qti')

    return client, store_id

# æŸ¥è©¢å‡½æ•¸
def query_penalties(client: genai.Client, query: str, store_id: str, model: str = 'gemini-2.5-flash', filters: dict = None) -> dict:
    """
    ä½¿ç”¨ Gemini File Search Store æŸ¥è©¢è£ç½°æ¡ˆä»¶

    Args:
        query: æŸ¥è©¢æ–‡å­—
        store_id: Gemini Store ID
        filters: ç¯©é¸æ¢ä»¶ï¼ˆæ—¥æœŸç¯„åœã€ä¾†æºå–®ä½ç­‰ï¼‰

    Returns:
        æŸ¥è©¢çµæœå­—å…¸
    """
    try:
        # å»ºç«‹ç³»çµ±æŒ‡ä»¤ï¼ˆé‡å°è£ç½°æ¡ˆä»¶çš„æ™‚æ•ˆæ€§å„ªåŒ–ï¼‰
        system_instruction = """ä½ æ˜¯é‡‘èç›£ç£ç®¡ç†å§”å“¡æœƒçš„è£ç½°æ¡ˆä»¶æŸ¥è©¢åŠ©æ‰‹ã€‚

ã€é‡è¦ã€‘æ™‚æ•ˆæ€§èˆ‡ç¨ç«‹æ€§è¦å‰‡ï¼š

1. **è£ç½°æ¡ˆä»¶ç‰¹æ€§**ï¼š
   - æ¯å€‹è£ç½°æ¡ˆä»¶éƒ½æ˜¯ç¨ç«‹çš„æ­·å²è¨˜éŒ„
   - ä¸åŒæ—¥æœŸçš„æ¡ˆä»¶ä¸äº’ç›¸å–ä»£
   - å¯å¼•ç”¨å¤šå€‹æ¡ˆä»¶ä½œç‚ºåƒè€ƒ
   - æŒ‰æ—¥æœŸæˆ–ç›¸é—œæ€§æ’åº

2. **æŸ¥è©¢å„ªå…ˆé †åº**ï¼ˆç•¶æœ‰å¤šç­†ç›¸é—œæ¡ˆä»¶æ™‚ï¼‰ï¼š
   - å„ªå…ˆåˆ—å‡º**æœ€è¿‘**çš„æ¡ˆä»¶ï¼ˆæ—¥æœŸè¶Šæ–°è¶Šå„ªå…ˆï¼‰
   - åŒæ™‚åƒè€ƒç›¸ä¼¼é•è¦é¡å‹çš„æ­·å²æ¡ˆä»¶
   - å¦‚æœä½¿ç”¨è€…æ˜ç¢ºè¦æ±‚ç‰¹å®šæ™‚é–“ç¯„åœï¼Œåš´æ ¼éµå®ˆ

3. **å›ç­”æ ¼å¼è¦æ±‚**ï¼š
   - æä¾›å…·é«”çš„æ¡ˆä»¶è³‡è¨Šï¼ˆæ—¥æœŸã€å–®ä½ã€è¢«è™•ç½°å°è±¡ã€é•è¦äº‹é …ã€è£ç½°é‡‘é¡ã€æ³•å¾‹ä¾æ“šï¼‰
   - å§‹çµ‚è¨»æ˜**ç™¼æ–‡æ—¥æœŸ**å’Œ**ç™¼æ–‡å­—è™Ÿ**
   - **é‡è¦ï¼šä¸è¦åœ¨å›ç­”ä¸­åˆ—å‡ºã€Œè³‡æ–™ä¾†æºã€æˆ–æª”å**ï¼ˆç³»çµ±æœƒè‡ªå‹•é¡¯ç¤ºåƒè€ƒæ–‡ä»¶ï¼‰
   - ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œä¿æŒå°ˆæ¥­ä½†æ˜“æ‡‚çš„èªæ°£
   - å¦‚æœæ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ï¼Œè«‹æ˜ç¢ºå‘ŠçŸ¥

4. **å¤šæ¡ˆä»¶è™•ç†**ï¼š
   - å¦‚æœæœ‰å¤šç­†ç›¸é—œæ¡ˆä»¶ï¼Œåˆ—å‡ºå‰ 3-5 ç­†æœ€ç›¸é—œçš„
   - æŒ‰æ™‚é–“é †åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰æˆ–ç›¸é—œæ€§æ’åº
   - æ¯å€‹æ¡ˆä»¶ç¨ç«‹èªªæ˜ï¼Œä¸è¦æ··æ·†

5. **æ¦‚å¿µæ€§å•é¡Œè™•ç†**ï¼ˆé‡è¦ï¼‰ï¼š
   - ç•¶ä½¿ç”¨è€…æå‡ºæ¦‚å¿µæ€§å•é¡Œï¼ˆå¦‚ã€Œé­è£ç½°å¾Œæœ‰å“ªäº›æ¥­å‹™é™åˆ¶ã€ï¼‰ï¼Œå¯ä»¥æä¾›ç¸½çµå¼å›ç­”
   - **ä½†ä»å»ºè­°åˆ—å‡ºè‡³å°‘ 1-2 å€‹å…·é«”æ¡ˆä¾‹**ä½œç‚ºèªªæ˜ï¼Œä½¿ç”¨ä¸Šè¿°æ ¼å¼
   - ä¾‹å¦‚ï¼šå…ˆç¸½çµæ¥­å‹™é™åˆ¶é¡å‹ï¼Œå†åˆ—å‡ºã€Œ### 1. [å…·é«”æ¡ˆä¾‹]ã€
   - é€™æ¨£æ—¢èƒ½å›ç­”æ¦‚å¿µå•é¡Œï¼Œä¹Ÿèƒ½è®“ä½¿ç”¨è€…åƒè€ƒå¯¦éš›æ¡ˆä¾‹
   - å¦‚æœé¸æ“‡åªæä¾›ç¸½çµè€Œä¸åˆ—å‡ºæ¡ˆä¾‹ï¼Œç³»çµ±æœƒåœ¨ã€Œä¹Ÿå¯ä»¥å¦å¤–åƒè€ƒã€å€å¡Šé¡¯ç¤ºç›¸é—œæ¡ˆä»¶

å›ç­”æ ¼å¼ç¯„ä¾‹ï¼š

## æŸ¥è©¢çµæœ

æ‰¾åˆ° X ç­†ç›¸é—œè£ç½°æ¡ˆä»¶ï¼š

### 1. [æ¡ˆä»¶æ¨™é¡Œ]ï¼ˆæœ€æ–°ï¼‰
- **æ—¥æœŸ**ï¼šYYYY-MM-DD
- **ç™¼æ–‡å­—è™Ÿ**ï¼šé‡‘ç®¡XXå­—ç¬¬XXXXXXXXXè™Ÿ
- **ä¾†æºå–®ä½**ï¼šXXXå±€
- **è¢«è™•ç½°å°è±¡**ï¼šXXXå…¬å¸/éŠ€è¡Œ/ä¿éšª
- **é•è¦äº‹é …**ï¼š[ç°¡è¦èªªæ˜]
- **è£ç½°é‡‘é¡**ï¼šæ–°è‡ºå¹£ XXX è¬å…ƒ
- **æ³•å¾‹ä¾æ“š**ï¼š[ç›¸é—œæ³•è¦æ¢æ–‡]

ï¼ˆæ³¨æ„ï¼šä¸è¦åœ¨æ¯å€‹æ¡ˆä»¶å¾Œé¢åŠ ä¸Šã€Œè³‡æ–™ä¾†æºã€æˆ–æª”åï¼Œç³»çµ±æœƒè‡ªå‹•åœ¨æœ€ä¸‹æ–¹é¡¯ç¤ºåƒè€ƒæ–‡ä»¶ï¼‰
"""

        # å»ºç«‹å®Œæ•´æŸ¥è©¢ï¼ˆç¯©é¸æ¢ä»¶ï¼‰
        full_query = query

        if filters:
            filter_parts = []

            if filters.get('start_date') and filters.get('end_date'):
                filter_parts.append(
                    f"æ—¥æœŸç¯„åœï¼š{filters['start_date']} åˆ° {filters['end_date']}"
                )

            if filters.get('source_units'):
                units_str = "ã€".join(filters['source_units'])
                filter_parts.append(f"ä¾†æºå–®ä½ï¼š{units_str}")

            if filters.get('min_penalty'):
                filter_parts.append(f"è£ç½°é‡‘é¡è‡³å°‘ï¼š{filters['min_penalty']:,} å…ƒ")

            if filter_parts:
                full_query += "\n\nç¯©é¸æ¢ä»¶ï¼š\n" + "\n".join(f"- {p}" for p in filter_parts)

        # æ ¹æ“šæ¨¡å‹é¡å‹è¨­å®š token é™åˆ¶
        # Pro æ¨¡å‹é€šå¸¸æä¾›æ›´è©³ç´°çš„å›ç­”ï¼Œéœ€è¦æ›´å¤š tokens
        max_tokens = 8192 if 'pro' in model.lower() else 4096

        # ä½¿ç”¨ File Search Store é€²è¡ŒæŸ¥è©¢ï¼ˆä½¿ç”¨æ­£ç¢ºçš„å‹åˆ¥ç‰©ä»¶ï¼‰
        response = client.models.generate_content(
            model=model,  # ä½¿ç”¨ç”¨æˆ¶é¸æ“‡çš„æ¨¡å‹
            contents=full_query,
            config=types.GenerateContentConfig(
                tools=[
                    types.Tool(
                        file_search=types.FileSearch(
                            file_search_store_names=[store_id]
                        )
                    )
                ],
                temperature=0.1,
                max_output_tokens=max_tokens,
                system_instruction=system_instruction
            )
        )

        # æå–ä¾†æºæ–‡ä»¶
        sources = []
        seen_files = {}  # ç”¨æ–¼å»é‡

        if hasattr(response, 'candidates') and len(response.candidates) > 0:
            candidate = response.candidates[0]

            if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                metadata = candidate.grounding_metadata

                # å„ªå…ˆå¾ grounding_supports æå–ï¼ˆåŒ…å«å¼•ç”¨è³‡è¨Šï¼‰
                if hasattr(metadata, 'grounding_supports') and metadata.grounding_supports:
                    for support in metadata.grounding_supports:
                        if hasattr(support, 'grounding_chunk_indices'):
                            for chunk_idx in support.grounding_chunk_indices:
                                if chunk_idx < len(metadata.grounding_chunks):
                                    chunk = metadata.grounding_chunks[chunk_idx]

                                    if hasattr(chunk, 'retrieved_context'):
                                        context = chunk.retrieved_context

                                        # æå–æ–‡ä»¶ ID/åç¨±
                                        filename = "æœªçŸ¥æ–‡ä»¶"
                                        if hasattr(context, 'title') and context.title:
                                            filename = context.title
                                        elif hasattr(context, 'uri') and context.uri:
                                            filename = context.uri.split('/')[-1]

                                        # æå–å…§å®¹ç‰‡æ®µ
                                        snippet = ""
                                        if hasattr(context, 'text') and context.text:
                                            snippet = context.text

                                        # ä½¿ç”¨ snippet çš„éƒ¨åˆ†å…§å®¹ä½œç‚ºå”¯ä¸€æ¨™è­˜é¿å…é‡è¤‡
                                        snippet_id = snippet[:100] if snippet else str(len(sources))

                                        if snippet_id not in seen_files:
                                            sources.append({
                                                'filename': filename,
                                                'snippet': snippet
                                            })
                                            seen_files[snippet_id] = True

                # å¦‚æœæ²’æœ‰ grounding_supportsï¼Œå›é€€åˆ° grounding_chunks
                if not sources and hasattr(metadata, 'grounding_chunks') and metadata.grounding_chunks:
                    for chunk in metadata.grounding_chunks:
                        if hasattr(chunk, 'retrieved_context'):
                            context = chunk.retrieved_context

                            # æå–æ–‡ä»¶ ID/åç¨±
                            filename = "æœªçŸ¥æ–‡ä»¶"
                            if hasattr(context, 'title') and context.title:
                                filename = context.title
                            elif hasattr(context, 'uri') and context.uri:
                                filename = context.uri.split('/')[-1]

                            # æå–å…§å®¹ç‰‡æ®µ
                            snippet = ""
                            if hasattr(context, 'text') and context.text:
                                snippet = context.text

                            # ä½¿ç”¨ snippet çš„éƒ¨åˆ†å…§å®¹ä½œç‚ºå”¯ä¸€æ¨™è­˜é¿å…é‡è¤‡
                            snippet_id = snippet[:100] if snippet else str(len(sources))

                            if snippet_id not in seen_files:
                                sources.append({
                                    'filename': filename,
                                    'snippet': snippet
                                })
                                seen_files[snippet_id] = True

        return {
            'success': True,
            'text': response.text,
            'sources': sources
        }

    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

# ä¸»æ‡‰ç”¨
def main():
    """ä¸»æ‡‰ç”¨ç¨‹å¼"""

    # æ¨™é¡Œ
    st.title("âš–ï¸ é‡‘ç®¡æœƒè£ç½°æ¡ˆä»¶æŸ¥è©¢ç³»çµ±")
    st.markdown("æŸ¥è©¢ 2011-2025 å¹´é–“çš„é‡‘èæ©Ÿæ§‹è£ç½°æ¡ˆä»¶ï¼ˆå…± 495 ç­†ï¼‰")
    st.info("ğŸ’¡ æœ¬ç³»çµ±ç‚ºå±•ç¤ºç”¨ï¼Œå¦‚é‡ç•«é¢ç„¡åæ‡‰ï¼Œè«‹é‡æ–°æ•´ç†é é¢")

    # åˆå§‹åŒ– Gemini
    client, store_id = init_gemini()

    # å´é‚Šæ¬„ï¼šç¯©é¸æ¢ä»¶
    with st.sidebar:
        # æ¨¡å‹é¸æ“‡
        st.header("ğŸ¤– AI æ¨¡å‹")

        # é¡¯ç¤ºåç¨±åˆ° model ID çš„æ˜ å°„
        model_display_to_id = {
            "æ¨™æº–": "gemini-2.5-flash",
            "å°ˆæ¥­ï¼ˆè¼ƒæ…¢ï¼‰": "gemini-2.5-pro"
        }

        model_display = st.selectbox(
            "é¸æ“‡æ¨¡å‹",
            options=list(model_display_to_id.keys()),
            index=0,
            help="æ¨™æº–ï¼šé€Ÿåº¦å¿«ï¼›å°ˆæ¥­ï¼šæ›´æº–ç¢ºä½†è¼ƒæ…¢"
        )

        # è½‰æ›ç‚ºå¯¦éš›çš„ model ID
        model = model_display_to_id[model_display]

        st.divider()
        st.header("ğŸ” ç¯©é¸æ¢ä»¶")

        # æ—¥æœŸç¯„åœ
        st.subheader("æ—¥æœŸç¯„åœï¼ˆå¯é¸ï¼‰")
        enable_date_filter = st.checkbox("å•Ÿç”¨æ—¥æœŸç¯©é¸", value=False)

        if enable_date_filter:
            col1, col2 = st.columns(2)
            with col1:
                start_date = st.date_input(
                    "é–‹å§‹æ—¥æœŸ",
                    value=date(2020, 1, 1),
                    min_value=date(2011, 1, 1),
                    max_value=date.today()
                )
            with col2:
                end_date = st.date_input(
                    "çµæŸæ—¥æœŸ",
                    value=date.today(),
                    min_value=date(2011, 1, 1),
                    max_value=date.today()
                )
        else:
            start_date = None
            end_date = None

        # ä¾†æºå–®ä½
        st.subheader("ä¾†æºå–®ä½")
        source_units = st.multiselect(
            "é¸æ“‡å–®ä½",
            options=["éŠ€è¡Œå±€", "ä¿éšªå±€", "è­‰åˆ¸æœŸè²¨å±€", "æª¢æŸ¥å±€"],
            default=[]
        )

        # è£ç½°é‡‘é¡
        st.subheader("è£ç½°é‡‘é¡")
        min_penalty = st.number_input(
            "æœ€ä½é‡‘é¡ï¼ˆå…ƒï¼‰",
            min_value=0,
            value=0,
            step=100000,
            format="%d"
        )

        # æ¸…é™¤ç¯©é¸
        if st.button("æ¸…é™¤æ‰€æœ‰ç¯©é¸", use_container_width=True):
            st.rerun()

        # é¡¯ç¤ºè³‡æ–™åº«è³‡è¨Š
        st.divider()
        st.caption("ğŸ“Š è³‡æ–™åº«è³‡è¨Š")
        st.caption(f"ç¸½æ¡ˆä»¶æ•¸ï¼š495 ç­†")
        st.caption(f"æ—¥æœŸç¯„åœï¼š2011-11-09 è‡³ 2025-09-25")
        st.caption(f"éŠ€è¡Œå±€ï¼š225 ç­† (45.5%)")
        st.caption(f"ä¿éšªå±€ï¼š222 ç­† (44.8%)")
        st.caption(f"è­‰åˆ¸æœŸè²¨å±€ï¼š47 ç­† (9.5%)")

    # ä¸»è¦æŸ¥è©¢å€åŸŸ
    st.header("ğŸ’¬ æŸ¥è©¢")

    # ç¯„ä¾‹æŸ¥è©¢
    with st.expander("ğŸ’¡ æŸ¥è©¢ç¯„ä¾‹"):
        st.markdown("""
        **ä¸€èˆ¬æŸ¥è©¢**ï¼š
        - æœ€è¿‘æœ‰å“ªäº›éŠ€è¡Œè¢«è£ç½°ï¼Ÿ
        - 2024å¹´ä¿éšªæ¥­çš„è£ç½°æ¡ˆä»¶æœ‰å“ªäº›ï¼Ÿ
        - æŸ¥è©¢æ´—éŒ¢é˜²åˆ¶ç›¸é—œçš„è£ç½°æ¡ˆä»¶

        **ç‰¹å®šä¸»é¡Œ**ï¼š
        - å…§éƒ¨æ§åˆ¶ç¼ºå¤±çš„è£ç½°æ¡ˆä¾‹
        - å®¢æˆ¶è³‡æ–™å¤–æ´©ç›¸é—œè£ç½°
        - é•åè³‡è¨Šå®‰å…¨çš„æ¡ˆä»¶

        **é‡‘é¡æŸ¥è©¢**ï¼š
        - è£ç½°é‡‘é¡è¶…é 1000 è¬çš„æ¡ˆä»¶
        - é‡‘é¡æœ€é«˜çš„ 5 å€‹è£ç½°æ¡ˆä¾‹

        **è¶¨å‹¢åˆ†æ**ï¼š
        - 2023 vs 2024 å¹´éŠ€è¡Œå±€è£ç½°è¶¨å‹¢æ¯”è¼ƒ
        - æœ€å¸¸è¦‹çš„é•è¦é¡å‹æ˜¯ä»€éº¼ï¼Ÿ
        """)

    # åˆå§‹åŒ– session stateï¼ˆä½¿ç”¨ä¸åŒçš„è®Šæ•¸åï¼‰
    if 'current_query' not in st.session_state:
        st.session_state.current_query = ""

    # æŸ¥è©¢è¼¸å…¥
    query = st.text_area(
        "è«‹è¼¸å…¥æŸ¥è©¢å…§å®¹ï¼š",
        value=st.session_state.current_query,
        placeholder="ä¾‹å¦‚ï¼š2024å¹´æœ‰å“ªäº›éŠ€è¡Œå› ç‚ºæ´—éŒ¢é˜²åˆ¶è¢«è£ç½°ï¼Ÿ",
        height=100
    )

    # å¿«é€ŸæŸ¥è©¢æŒ‰éˆ•
    st.markdown("#### ğŸš€ å¿«é€ŸæŸ¥è©¢")

    quick_queries = [
        "é•åé‡‘æ§æ³•åˆ©å®³é—œä¿‚äººè¦å®šæœƒå—åˆ°ä»€éº¼è™•ç½°ï¼Ÿ",
        "è«‹å•åœ¨è­‰åˆ¸å› ç‚ºå°ˆæ¥­æŠ•è³‡äººè³‡æ ¼å¯©æ ¸çš„è£ç½°æœ‰å“ªäº›ï¼Ÿ",
        "è¾¦ç†å…±åŒè¡ŒéŠ·è¢«è£ç½°çš„æ¡ˆä¾‹æœ‰å“ªäº›ï¼Ÿ",
        "é‡‘ç®¡æœƒå°å‰µæŠ•å…¬å¸çš„è£ç½°æœ‰å“ªäº›ï¼Ÿ",
        "è­‰åˆ¸å•†é­ä¸»ç®¡æ©Ÿé—œè£ç½°ã€Œè­¦å‘Šã€è™•åˆ†ï¼Œæœ‰å“ªäº›æ¥­å‹™æœƒå—é™åˆ¶ï¼Ÿ",
        "å…§ç·šäº¤æ˜“æœ‰ç½ªåˆ¤æ±ºæ‰€èªå®šé‡å¤§è¨Šæ¯æˆç«‹çš„æ™‚é»"
    ]

    cols = st.columns(2)
    for idx, quick_query in enumerate(quick_queries):
        col_idx = idx % 2
        with cols[col_idx]:
            if st.button(f"ğŸ“Œ {quick_query}", key=f"quick_{idx}", use_container_width=True):
                st.session_state.current_query = quick_query
                st.rerun()

    st.markdown("")  # ç©ºè¡Œåˆ†éš”

    # æŸ¥è©¢æŒ‰éˆ•
    col1, col2, col3 = st.columns([1, 1, 4])
    with col1:
        search_button = st.button("ğŸ” æŸ¥è©¢", type="primary", use_container_width=True)
    with col2:
        clear_button = st.button("ğŸ—‘ï¸ æ¸…é™¤", use_container_width=True)

    if clear_button:
        st.session_state.current_query = ""
        st.rerun()

    # åŸ·è¡ŒæŸ¥è©¢
    if search_button and query:
        with st.spinner("ğŸ” æŸ¥è©¢ä¸­..."):
            # æº–å‚™ç¯©é¸æ¢ä»¶
            filters = {}

            if start_date and end_date:
                filters['start_date'] = start_date.strftime('%Y-%m-%d')
                filters['end_date'] = end_date.strftime('%Y-%m-%d')

            if source_units:
                filters['source_units'] = source_units

            if min_penalty > 0:
                filters['min_penalty'] = min_penalty

            # åŸ·è¡ŒæŸ¥è©¢
            result = query_penalties(client, query, store_id, model, filters)

            # é¡¯ç¤ºçµæœ
            if result['success']:
                st.success("âœ… æŸ¥è©¢å®Œæˆ")

                # è¼‰å…¥æ˜ å°„æª”ï¼ˆç”¨æ–¼æ³•æ¢é€£çµå’ŒåŸå§‹é€£çµï¼‰
                mapping = load_file_mapping()
                gemini_id_mapping = load_gemini_id_mapping()

                # æ”¶é›†æ‰€æœ‰åƒè€ƒæ–‡ä»¶ä¸­çš„æ³•æ¢é€£çµï¼ˆéæ¿¾ç„¡æ•ˆæ³•æ¢ï¼‰
                all_law_links = {}
                if result.get('sources') and len(result['sources']) > 0:
                    for source in result['sources']:
                        filename = source.get('filename', '')
                        file_id = extract_file_id(filename, gemini_id_mapping)
                        file_info = mapping.get(file_id, {})
                        law_links = file_info.get('law_links', {})
                        # éæ¿¾æ‰ç„¡æ•ˆæ³•æ¢ï¼ˆä»¥ã€Œèˆ‡ã€ã€ŒåŒã€ç­‰é–‹é ­çš„èª¤åŒ¹é…ï¼‰
                        filtered_law_links = {
                            law: link for law, link in law_links.items()
                            if not law.startswith(('èˆ‡', 'åŒ', 'åŠ', 'æˆ–', 'å’Œ'))
                        }
                        # åˆä½µæ³•æ¢é€£çµ
                        all_law_links.update(filtered_law_links)

                # å€å¡Š1ï¼šé¡¯ç¤ºå›æ‡‰ï¼ˆç‚ºæ³•æ¢å’Œæ¡ˆä»¶æ¨™é¡ŒåŠ å…¥é€£çµï¼‰
                st.markdown("---")
                response_text = result['text']

                # å…ˆè¨ˆç®—å›ç­”ä¸­æœ‰å¤šå°‘å€‹æ¨™é¡Œï¼ˆ### 1. xxxï¼‰
                import re
                title_pattern = r'###\s*\d+\.\s+[^\n]+'
                title_matches = re.findall(title_pattern, response_text)
                num_titles = len(title_matches)

                # å¾ sources æå–æ¡ˆä»¶é€£çµï¼ˆåªå–å‰ num_titles å€‹ï¼Œå°æ‡‰æœ‰æ¨™é¡Œçš„æ¡ˆä»¶ï¼‰
                case_urls = []
                seen_file_ids = set()
                count = 0

                for source in result.get('sources', []):
                    # åªè™•ç†æœ‰æ¨™é¡Œçš„æ¡ˆä»¶æ•¸é‡
                    if count >= num_titles:
                        break

                    filename = source.get('filename', '')
                    file_id = extract_file_id(filename, gemini_id_mapping)

                    # å»é‡ï¼ˆæ¯å€‹æ–‡ä»¶åªå–ç¬¬ä¸€æ¬¡å‡ºç¾ï¼‰
                    if file_id and file_id not in seen_file_ids:
                        file_info = mapping.get(file_id, {})
                        detail_url = file_info.get('original_url', '')
                        if detail_url:
                            case_urls.append(detail_url)
                            seen_file_ids.add(file_id)
                            count += 1

                # ç‚ºæ¡ˆä»¶æ¨™é¡ŒåŠ å…¥é€£çµ
                response_with_case_links = insert_case_links_by_order(response_text, case_urls)

                # ç‚ºæ³•æ¢åŠ å…¥é€£çµ
                response_with_all_links = add_law_links_to_text(response_with_case_links, all_law_links)

                st.markdown(response_with_all_links)

                # ===== å€å¡Š2ï¼šç›¸é—œè£ç½°æ¡ˆä»¶åŸå§‹å…¬å‘Šï¼ˆå·²è¨»è§£ï¼‰ =====
                # è¨»è§£åŸå› ï¼šåŠŸèƒ½å·²æ•´åˆåˆ°å€å¡Š1çš„æ¨™é¡Œé€£çµ
                # ä¿ç•™ç¨‹å¼ç¢¼ä¾›æœªä¾†åƒè€ƒ
                #
                # if result.get('sources') and len(result['sources']) > 0:
                #     # æ”¶é›†æ‰€æœ‰åŸå§‹é€£çµï¼ˆå»é‡ï¼‰
                #     original_urls = []
                #     seen_urls = set()
                #
                #     for source in result['sources']:
                #         filename = source.get('filename', '')
                #         file_id = extract_file_id(filename, gemini_id_mapping)
                #         file_info = mapping.get(file_id, {})
                #         url = file_info.get('original_url', '')
                #
                #         if url and url not in seen_urls:
                #             original_urls.append({
                #                 'url': url,
                #                 'display_name': file_info.get('display_name', file_id)
                #             })
                #             seen_urls.add(url)
                #
                #     # é¡¯ç¤ºåŸå§‹é€£çµ
                #     if original_urls:
                #         st.markdown("---")
                #         st.markdown("**ğŸ”— ç›¸é—œè£ç½°æ¡ˆä»¶åŸå§‹å…¬å‘Š**")
                #         for item in original_urls:
                #             st.markdown(f"- [{item['display_name']}]({item['url']})")

                # ===== å€å¡Š3ï¼šä¹Ÿå¯ä»¥å¦å¤–åƒè€ƒï¼ˆæ–°ç‰ˆï¼‰ =====
                # åªé¡¯ç¤ºä¸åœ¨æŸ¥è©¢çµæœæ¨™é¡Œä¸­çš„é¡å¤–åƒè€ƒæ–‡ä»¶
                if result.get('sources') and len(result['sources']) > 0:
                    st.markdown("---")
                    display_grounding_sources_v2(
                        sources=result['sources'],
                        file_mapping=mapping,
                        gemini_id_mapping=gemini_id_mapping,
                        excluded_file_ids=seen_file_ids  # æ’é™¤å·²åœ¨å€å¡Š1æ¨™é¡Œä¸­çš„æ–‡ä»¶
                    )

                # ===== é™¤éŒ¯è³‡è¨Šï¼šé¡¯ç¤ºåŸå§‹åƒè€ƒå…§å®¹åˆ—è¡¨ =====
                # ç§»åˆ°æ¢ä»¶å¤–ï¼Œå³ä½¿ sources ç‚ºç©ºä¹Ÿé¡¯ç¤ºï¼ˆç”¨æ–¼è¨ºæ–·å•é¡Œï¼‰
                st.markdown("---")
                with st.expander("ğŸ” é™¤éŒ¯è³‡è¨Šï¼šGemini åŸå§‹åƒè€ƒåˆ—è¡¨", expanded=False):
                        # è¨ºæ–·è³‡è¨Šï¼šæª¢æŸ¥ sources æ˜¯å¦å­˜åœ¨
                        sources = result.get('sources', [])

                        # æ•´åˆé¡¯ç¤º sources æ•¸é‡ï¼ˆæœªå»é‡ï¼‰å’ŒåŸå§‹åˆ—è¡¨
                        with st.expander(f"ğŸ“Š Gemini è¿”å›çš„ sources æ•¸é‡ï¼ˆæœªå»é‡ï¼‰: {len(sources)}", expanded=False):
                            if sources:
                                for i, source in enumerate(sources, 1):
                                    filename = source.get('filename', 'N/A')
                                    file_id = extract_file_id(filename, gemini_id_mapping)
                                    st.caption(f"{i}. Gemini ID: `{filename}` â†’ File ID: `{file_id}`")
                            else:
                                st.caption("ç„¡ sources")

                        st.info(f"ğŸ“ å›ç­”ä¸­çš„æ¨™é¡Œæ•¸é‡: {num_titles}")
                        st.info(f"âœ… åŠ å…¥æŸ¥è©¢çµæœçš„æ–‡ä»¶æ•¸é‡: {len(seen_file_ids)}")

                        if not sources:
                            st.warning("âš ï¸ Gemini æœªè¿”å›ä»»ä½•åƒè€ƒæ–‡ä»¶ï¼ˆsources ç‚ºç©ºï¼‰")
                            st.caption("å¯èƒ½åŸå› ï¼š")
                            st.caption("1. Gemini å›æ‡‰è¢«æˆªæ–·ï¼Œå°è‡´ sources è³‡è¨Šéºå¤±")
                            st.caption("2. File Search Store æŸ¥è©¢å¤±æ•—")
                            st.caption("3. å›æ‡‰è™•ç†é‚è¼¯éŒ¯èª¤")
                        else:
                            # æå–ä¸¦å»é‡æ‰€æœ‰ file_idsï¼ˆåŒ…å«æ˜ å°„å¤±æ•—çš„ï¼‰
                            all_file_ids = []
                            failed_mappings = []
                            seen_debug = set()

                            for source in sources:
                                filename = source.get('filename', '')
                                file_id = extract_file_id(filename, gemini_id_mapping)

                                # æª¢æŸ¥æ˜¯å¦æ˜ å°„æˆåŠŸ
                                if file_id and file_id not in seen_debug:
                                    # æª¢æŸ¥æ˜¯å¦åœ¨ file_mapping ä¸­
                                    if file_id in mapping:
                                        all_file_ids.append(file_id)
                                        seen_debug.add(file_id)
                                    else:
                                        # æ˜ å°„å¤±æ•—ï¼ˆfile_id ä¸åœ¨ file_mapping ä¸­ï¼‰
                                        failed_mappings.append({'filename': filename, 'file_id': file_id})
                                elif not file_id and filename not in [f['filename'] for f in failed_mappings]:
                                    # å®Œå…¨ç„¡æ³•æå– file_id
                                    failed_mappings.append({'filename': filename, 'file_id': None})

                            st.write(f"**ç¸½å…± {len(all_file_ids)} ç­†æœ‰æ•ˆåƒè€ƒæ–‡ä»¶ï¼š**")

                            for i, file_id in enumerate(all_file_ids, 1):
                                file_info = mapping.get(file_id, {})
                                display_name = file_info.get('display_name', file_id)

                                # æ¨™è¨»æ˜¯å¦å·²åœ¨æŸ¥è©¢çµæœä¸­
                                if file_id in seen_file_ids:
                                    st.write(f"{i}. ğŸ“„ {display_name} âœ… *ï¼ˆå·²åœ¨æŸ¥è©¢çµæœä¸­ï¼‰*")
                                else:
                                    st.write(f"{i}. ğŸ“„ {display_name} â­ *ï¼ˆé¡å¤–åƒè€ƒï¼‰*")

                            # é¡¯ç¤ºæ˜ å°„å¤±æ•—çš„æª”æ¡ˆ
                            if failed_mappings:
                                st.warning(f"âš ï¸ **{len(failed_mappings)} ç­†æ˜ å°„å¤±æ•—ï¼ˆå·²è‡ªå‹•è·³éï¼‰ï¼š**")
                                for i, item in enumerate(failed_mappings, 1):
                                    filename = item['filename']
                                    file_id = item['file_id']
                                    if file_id:
                                        st.caption(f"{i}. Gemini ID: `{filename}` â†’ File ID: `{file_id}` (ä¸åœ¨ file_mapping ä¸­)")
                                    else:
                                        st.caption(f"{i}. Gemini ID: `{filename}` (ç„¡æ³•æå– file_id)")
            else:
                st.error(f"âŒ æŸ¥è©¢å¤±æ•—ï¼š{result['error']}")

    elif search_button and not query:
        st.warning("âš ï¸ è«‹è¼¸å…¥æŸ¥è©¢å…§å®¹")

    # é å°¾
    st.divider()
    st.caption("è³‡æ–™ä¾†æºï¼šé‡‘èç›£ç£ç®¡ç†å§”å“¡æœƒ")
    st.caption("âš ï¸ æœ¬ç³»çµ±åƒ…ä¾›åƒè€ƒï¼Œå¯¦éš›è£ç½°è³‡è¨Šè«‹ä»¥é‡‘ç®¡æœƒå®˜ç¶²å…¬å‘Šç‚ºæº–")

if __name__ == "__main__":
    main()
