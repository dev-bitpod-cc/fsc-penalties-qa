"""
FSC è£ç½°æ¡ˆä»¶æŸ¥è©¢ç³»çµ±
ä½¿ç”¨ Google Gemini File Search Store é€²è¡Œ RAG æŸ¥è©¢

Version: 1.3.3 - åœ¨é å°¾é¡¯ç¤ºç‰ˆæœ¬è™Ÿ (2025-11-20)
  - ğŸ·ï¸ åœ¨é é¢å·¦ä¸‹è§’é¡¯ç¤ºç‰ˆæœ¬è™Ÿ (v1.3.3)
  - ğŸ“ ä½¿ç”¨å…©æ¬„ä½ˆå±€ï¼šå·¦é‚Šç‰ˆæœ¬è™Ÿï¼Œå³é‚Šè³‡æ–™ä¾†æº

Previous: 1.3.2 (2025-11-20)
  - å®Œå…¨ç§»é™¤ä¸­é–“æ¨™é¡Œï¼Œä¿æŒæµæš¢æ®µè½
  - åªæœ‰å…·é«”æ¡ˆä¾‹æ‰ä½¿ç”¨ ### æ¨™é¡Œ

Previous: 1.2.0 (2025-11-19)
  - ç°¡åŒ– UIï¼ˆåƒè€ƒ Sanction-Deploy é¢¨æ ¼ï¼‰+ Plain Text Store
  - æŒ‡æ¨™æ¬„åªé¡¯ç¤ºï¼šä¾†æºæ•¸é‡
  - Plain Text Store: 490 ç­†è³‡æ–™ï¼Œ100% pcode æ˜ å°„è¦†è“‹ç‡
"""

import os
import streamlit as st
from datetime import datetime, date
from dotenv import load_dotenv
from google import genai
from google.genai import types

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# è¼‰å…¥æ˜ å°„æª”
def load_file_mapping():
    """è¼‰å…¥æª”æ¡ˆæ˜ å°„æª”ï¼ˆç§»é™¤å¿«å–ä»¥ç¢ºä¿å§‹çµ‚ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬ï¼‰"""
    from pathlib import Path
    import os

    mapping_file = Path(__file__).parent / 'data/penalties/file_mapping.json'

    if not mapping_file.exists():
        return {}

    try:
        import json
        with open(mapping_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # é¡¯ç¤ºæª”æ¡ˆè³‡è¨Šä¾›é™¤éŒ¯
        file_mtime = os.path.getmtime(mapping_file)
        file_size = os.path.getsize(mapping_file) / (1024 * 1024)  # MB
        # st.sidebar.text(f"ğŸ“„ file_mapping.json\næ›´æ–°æ™‚é–“: {datetime.fromtimestamp(file_mtime).strftime('%Y-%m-%d %H:%M:%S')}\nå¤§å°: {file_size:.2f} MB")

        return data
    except Exception as e:
        st.warning(f"âš ï¸ è¼‰å…¥æ˜ å°„æª”å¤±æ•—: {e}")
        return {}

def load_gemini_id_mapping():
    """è¼‰å…¥ Gemini ID åå‘æ˜ å°„æª”ï¼ˆGemini file_id â†’ file_idï¼‰ï¼ˆç§»é™¤å¿«å–ä»¥ç¢ºä¿å§‹çµ‚ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬ï¼‰"""
    from pathlib import Path
    mapping_file = Path(__file__).parent / 'data/penalties/gemini_id_mapping.json'

    if not mapping_file.exists():
        return {}

    try:
        import json
        with open(mapping_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        st.warning(f"âš ï¸ è¼‰å…¥ Gemini ID æ˜ å°„æª”å¤±æ•—: {e}")
        return {}

def extract_file_id(filename: str, gemini_id_mapping: dict = None) -> str:
    """å¾æª”åä¸­æå– file_id

    Args:
        filename: Gemini è¿”å›çš„æª”åï¼ˆå¯èƒ½æ˜¯å…§éƒ¨ ID å¦‚ "4ax547mbfiot"ï¼‰
        gemini_id_mapping: Gemini ID åå‘æ˜ å°„ (files/xxx â†’ fsc_pen_xxx)

    Returns:
        file_idï¼ˆç”¨æ–¼æŸ¥æ‰¾ file_mapping.jsonï¼‰ï¼Œå¦‚æœæ˜ å°„å¤±æ•—å‰‡è¿”å› None
    """
    import re

    # å¦‚æœæœ‰ gemini_id_mappingï¼Œå…ˆå˜—è©¦åå‘æŸ¥æ‰¾
    if gemini_id_mapping:
        # å˜—è©¦å®Œæ•´ IDï¼ˆfiles/xxxï¼‰
        full_id = f"files/{filename.replace('files/', '')}"
        if full_id in gemini_id_mapping:
            return gemini_id_mapping[full_id]

    # å›é€€ï¼šå¾æª”åæå–ï¼ˆé©ç”¨æ–¼èˆŠè³‡æ–™æˆ–ç›´æ¥æ˜¯æª”åçš„æƒ…æ³ï¼‰
    # ç§»é™¤ files/ å‰ç¶´å’Œ .md å¾Œç¶´
    filename_clean = filename.replace('files/', '').replace('.md', '')

    # æå– fsc_pen_YYYYMMDD_NNNN æ ¼å¼
    match = re.match(r'(fsc_pen_\d{8}_\d{4})', filename_clean)
    if match:
        return match.group(1)

    # å¦‚æœç„¡æ³•æå–æœ‰æ•ˆçš„ file_idï¼Œè¿”å› Noneï¼ˆé¿å…ä½¿ç”¨ç„¡æ•ˆçš„ Gemini å…§éƒ¨ IDï¼‰
    return None

def add_law_links_to_text(text: str, law_links_dict: dict) -> str:
    """åœ¨æ–‡å­—ä¸­ç‚ºæ³•æ¢åŠ å…¥é€£çµ

    Args:
        text: åŸå§‹æ–‡å­—
        law_links_dict: æ³•æ¢åˆ°é€£çµçš„æ˜ å°„ {æ³•æ¢åç¨±: URL}

    Returns:
        åŠ å…¥é€£çµå¾Œçš„æ–‡å­—
    """
    import re

    if not law_links_dict:
        return text

    # æŒ‰æ³•æ¢åç¨±é•·åº¦æ’åºï¼ˆé•·çš„å„ªå…ˆï¼Œé¿å…çŸ­çš„å…ˆè¢«æ›¿æ›å°è‡´é•·çš„ç„¡æ³•åŒ¹é…ï¼‰
    sorted_laws = sorted(law_links_dict.keys(), key=len, reverse=True)

    result = text
    replaced_positions = set()  # è¨˜éŒ„å·²æ›¿æ›çš„ä½ç½®ï¼Œé¿å…é‡è¤‡æ›¿æ›

    # === ç¬¬ä¸€éšæ®µï¼šè™•ç†å®Œæ•´æ³•æ¢åç¨± ===
    for law in sorted_laws:
        # è·³éç°¡å¯«å½¢å¼ï¼ˆç•™å¾…ç¬¬äºŒéšæ®µè™•ç†ï¼‰
        if law.startswith('ç¬¬'):
            continue

        link = law_links_dict[law]

        # æå–æ³•å¾‹åç¨±å’Œæ¢è™Ÿ
        law_match = re.match(r'^(.+?)(ç¬¬\d+æ¢(?:ä¹‹\d+)?)', law)
        if not law_match:
            continue

        law_name = law_match.group(1)  # ä¾‹å¦‚ï¼šã€Œé‡‘èæ§è‚¡å…¬å¸æ³•ã€
        article = law_match.group(2)   # ä¾‹å¦‚ï¼šã€Œç¬¬45æ¢ã€

        # å»ºç«‹å½ˆæ€§åŒ¹é…æ¨¡å¼ï¼šæ”¯æ´æ›¸åè™Ÿã€é …/æ¬¾/ç›®ã€å‰ç½®é€£æ¥è©
        law_name_escaped = re.escape(law_name)
        article_escaped = re.escape(article)

        # åŒ¹é…æ¨¡å¼ï¼šå¯é¸çš„å‰ç½®é€£æ¥è© + æ³•å¾‹åç¨± + æ¢è™Ÿ + é …/æ¬¾/ç›®
        pattern = (
            r'(?<!\[)(?<!\()'  # ä¸åœ¨é€£çµä¸­
            r'(?:[ã€ï¼ŒåŠèˆ‡å’Œä»¥]\s*)?'  # å¯é¸çš„å‰ç½®é€£æ¥è©
            r'(?:ã€Š)?' + law_name_escaped + r'(?:ã€‹)?'  # æ³•å¾‹åç¨±ï¼ˆå¯é¸æ›¸åè™Ÿï¼‰
            r'\s*' + article_escaped +  # æ¢è™Ÿ
            r'(?:ç¬¬\d+é …)?(?:ç¬¬\d+æ¬¾)?(?:ç¬¬\d+ç›®)?'  # å¯é¸çš„é …/æ¬¾/ç›®
            r'(?!\])(?!\))'  # ä¸åœ¨é€£çµä¸­
        )

        # æ‰¾åˆ°æ‰€æœ‰åŒ¹é…ä¸¦æ”¶é›†
        matches = []
        for match in re.finditer(pattern, result):
            start, end = match.span()

            # æª¢æŸ¥é€™å€‹ä½ç½®æ˜¯å¦å·²è¢«æ›¿æ›
            is_overlapping = False
            for pos, pos_end in replaced_positions:
                if (start < pos_end and end > pos):
                    is_overlapping = True
                    break

            if not is_overlapping:
                matched_text = match.group(0)
                matches.append((start, end, matched_text))

        # å¾å¾Œå¾€å‰æ›¿æ›ï¼ˆé¿å…ä½ç½®åç§»ï¼‰
        for start, end, matched_text in reversed(matches):
            # æª¢æŸ¥æ˜¯å¦æœ‰å‰ç½®é€£æ¥è©
            connector_match = re.match(r'^([ã€ï¼ŒåŠèˆ‡å’Œä»¥]\s*)?(.+)$', matched_text)
            if connector_match:
                connector = connector_match.group(1) or ''
                law_part = connector_match.group(2)
                replacement = f'{connector}[{law_part}]({link})'
            else:
                replacement = f'[{matched_text}]({link})'

            result = result[:start] + replacement + result[end:]
            new_end = start + len(replacement)
            replaced_positions.add((start, new_end))

    # === ç¬¬äºŒéšæ®µï¼šè™•ç†ç°¡å¯«å½¢å¼ï¼ˆå¦‚ã€Œã€ç¬¬51æ¢ã€ã€ŒåŠç¬¬60æ¢ã€ï¼‰ ===
    for law in sorted_laws:
        # åªè™•ç†ç°¡å¯«å½¢å¼
        if not law.startswith('ç¬¬'):
            continue

        link = law_links_dict[law]

        # åŒ¹é…ç°¡å¯«å½¢å¼ï¼šå‰é¢æœ‰ã€Œã€ã€ã€ŒåŠã€ã€Œèˆ‡ã€ã€Œå’Œã€ç­‰é€£æ¥è©
        article_escaped = re.escape(law)
        pattern = (
            r'(?<!\[)(?<!\()'  # ä¸åœ¨é€£çµä¸­
            r'(?:[ã€ï¼ŒåŠèˆ‡å’Œ])\s*' + article_escaped +  # é€£æ¥è© + æ¢è™Ÿ
            r'(?:ç¬¬\d+é …)?(?:ç¬¬\d+æ¬¾)?(?:ç¬¬\d+ç›®)?'  # å¯é¸çš„é …/æ¬¾/ç›®
            r'(?!\])(?!\))'  # ä¸åœ¨é€£çµä¸­
        )

        matches = []
        for match in re.finditer(pattern, result):
            start, end = match.span()

            # æª¢æŸ¥é€™å€‹ä½ç½®æ˜¯å¦å·²è¢«æ›¿æ›
            is_overlapping = False
            for pos, pos_end in replaced_positions:
                if (start < pos_end and end > pos):
                    is_overlapping = True
                    break

            if not is_overlapping:
                matched_text = match.group(0)
                # ä¿ç•™å‰é¢çš„é€£æ¥è©
                matches.append((start, end, matched_text))

        # å¾å¾Œå¾€å‰æ›¿æ›
        for start, end, matched_text in reversed(matches):
            # æå–é€£æ¥è©å’Œæ¢è™Ÿéƒ¨åˆ†
            connector_match = re.match(r'([ã€ï¼ŒåŠèˆ‡å’Œ]\s*)(.+)', matched_text)
            if connector_match:
                connector = connector_match.group(1)
                article_part = connector_match.group(2)
                replacement = f'{connector}[{article_part}]({link})'
                result = result[:start] + replacement + result[end:]
                new_end = start + len(replacement)
                replaced_positions.add((start, new_end))

    return result

def insert_case_links_by_order(text: str, case_urls: list) -> str:
    """
    æŒ‰é †åºå°‡æ¡ˆä»¶æ¨™é¡Œè½‰æ›ç‚ºé€£çµï¼ˆå€å¡Š1ç”¨ï¼‰

    Args:
        text: Gemini å›ç­”æ–‡å­—
        case_urls: æ¡ˆä»¶é€£çµåˆ—è¡¨ï¼ˆæŒ‰é †åºï¼Œå¾ grounding_metadata æå–ï¼‰

    Returns:
        æ’å…¥é€£çµå¾Œçš„æ–‡å­—
    """
    import re

    if not case_urls:
        return text

    # æ‰¾å‡ºæ‰€æœ‰æ¨™é¡Œï¼š### 1. [æ¨™é¡Œå…§å®¹]
    pattern = r'(###\s*\d+\.\s+)([^\n]+)'
    matches = list(re.finditer(pattern, text))

    if not matches:
        return text

    # å¾å¾Œå¾€å‰æ›¿æ›ï¼ˆé¿å…ä½ç½®åç§»ï¼‰
    result = text
    for i, match in enumerate(reversed(matches)):
        # åå‘ç´¢å¼•
        idx = len(matches) - 1 - i

        # æª¢æŸ¥æ˜¯å¦æœ‰å°æ‡‰çš„ URL
        if idx >= len(case_urls):
            continue

        prefix = match.group(1)      # "### 1. "
        title = match.group(2).strip()  # "ä¸‰å•†ç¾é‚¦äººå£½ä¿éšªè‚¡ä»½..."
        url = case_urls[idx]

        # æª¢æŸ¥æ˜¯å¦å·²ç¶“æ˜¯é€£çµï¼ˆé¿å…é‡è¤‡æ›¿æ›ï¼‰
        if title.startswith('[') and '](' in title:
            continue

        # æ›¿æ›ç‚ºé€£çµ
        new_text = f"{prefix}[{title}]({url})"
        result = result[:match.start()] + new_text + result[match.end():]

    return result

def remove_social_media_noise(text: str) -> str:
    """
    ç§»é™¤åŸå§‹æ–‡å­—ä¸­çš„ç¤¾ç¾¤åª’é«”åˆ†äº«æŒ‰éˆ•ç­‰é›œè¨Š

    Args:
        text: åŸå§‹æ–‡å­—

    Returns:
        æ¸…ç†å¾Œçš„æ–‡å­—
    """
    import re

    # ç¤¾ç¾¤åª’é«”ç›¸é—œé—œéµå­—
    noise_patterns = [
        r'facebook',
        r'Facebook',
        r'twitter',
        r'Twitter',
        r'line',
        r'LINE',
        r'åˆ†äº«',
        r'åˆ—å°',
        r'è½‰å¯„',
        r'å‹å–„åˆ—å°',
        r'å›ä¸Šä¸€é ',
        r':::',
        r'å›é¦–é ',
        r'ç¶²ç«™å°è¦½',
        r'English',
        r'å…’ç«¥ç‰ˆ',
        r'è¡Œå‹•ç‰ˆ',
        r'RSS',
        r'å­—ç´šå¤§å°',
        r'å° ä¸­ å¤§',
    ]

    # ç§»é™¤åŒ…å«é€™äº›é—œéµå­—çš„è¡Œ
    lines = text.split('\n')
    cleaned_lines = []

    for line in lines:
        line = line.strip()
        # è·³éç©ºè¡Œ
        if not line:
            continue

        # æª¢æŸ¥æ˜¯å¦åŒ…å«é›œè¨Šé—œéµå­—
        is_noise = False
        for pattern in noise_patterns:
            if re.search(pattern, line, re.IGNORECASE):
                is_noise = True
                break

        if not is_noise:
            cleaned_lines.append(line)

    return '\n'.join(cleaned_lines)

def display_sources_simple(sources: list, file_mapping: dict, gemini_id_mapping: dict):
    """
    ç°¡åŒ–ç‰ˆåƒè€ƒä¾†æºé¡¯ç¤º

    é¡¯ç¤º Gemini å›è¦†çš„æœ€æ¥è¿‘ chunk å…§å®¹å’ŒåŸå§‹é€£çµ

    Args:
        sources: å¾ query_penalties è¿”å›çš„ sources åˆ—è¡¨ï¼ˆåŒ…å« snippetï¼‰
        file_mapping: file_mapping.json çš„å…§å®¹
        gemini_id_mapping: Gemini ID æ˜ å°„
    """
    if not sources:
        st.warning("âš ï¸ æœªæ‰¾åˆ°åƒè€ƒä¾†æº")
        return

    # å»é‡ä¸¦æå–æœ‰æ•ˆçš„ file_idsï¼ŒåŒæ™‚ä¿å­˜å°æ‡‰çš„ snippet
    unique_sources = []
    seen = set()

    for source in sources:
        filename = source.get('filename', '')
        snippet = source.get('snippet', '')
        file_id = extract_file_id(filename, gemini_id_mapping)

        # è·³éæ˜ å°„å¤±æ•—æˆ–ä¸å­˜åœ¨æ–¼ file_mapping çš„æª”æ¡ˆ
        if not file_id or file_id not in file_mapping:
            continue

        if file_id not in seen:
            unique_sources.append({
                'file_id': file_id,
                'snippet': snippet
            })
            seen.add(file_id)

    if not unique_sources:
        st.warning("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„åƒè€ƒä¾†æº")
        return

    # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°â†’æœ€èˆŠï¼‰
    unique_sources.sort(
        key=lambda item: file_mapping.get(item['file_id'], {}).get('date', ''),
        reverse=True  # é™åºï¼šæœ€æ–°çš„åœ¨å‰é¢
    )

    # é¡¯ç¤ºåƒè€ƒä¾†æº
    st.subheader(f"ğŸ“š åƒè€ƒä¾†æº ({len(unique_sources)} ç­†ï¼Œä¾æ™‚é–“æ’åºï¼‰")

    for i, source_item in enumerate(unique_sources, 1):
        file_id = source_item['file_id']
        snippet = source_item['snippet']
        file_info = file_mapping.get(file_id, {})
        display_name = file_info.get('display_name', file_id)
        detail_url = file_info.get('original_url', '')

        # ä½¿ç”¨ expander é¡¯ç¤º
        with st.expander(f"ä¾†æº {i}: {display_name}", expanded=False):
            # é¡¯ç¤º Gemini æª¢ç´¢åˆ°çš„æœ€æ¥è¿‘ chunk å…§å®¹
            if snippet:
                st.markdown("**ğŸ“„ ç›¸é—œå…§å®¹ï¼š**")
                st.markdown(f"> {snippet}")
            else:
                st.info("ç„¡å¯ç”¨çš„å…§å®¹ç‰‡æ®µ")

            # åŸå§‹å…¬å‘Šé€£çµ
            if detail_url:
                st.markdown("---")
                st.markdown(f"ğŸ”— [æŸ¥çœ‹é‡‘ç®¡æœƒåŸå§‹å…¬å‘Š]({detail_url})")

# è¨­å®šé é¢
st.set_page_config(
    page_title="é‡‘ç®¡æœƒè£ç½°æ¡ˆä»¶æŸ¥è©¢ç³»çµ±",
    page_icon="âš–ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ– Gemini
@st.cache_resource
def init_gemini():
    """åˆå§‹åŒ– Gemini API"""
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        st.error("âŒ æ‰¾ä¸åˆ° GEMINI_API_KEYï¼Œè«‹è¨­å®šç’°å¢ƒè®Šæ•¸")
        st.stop()

    # å»ºç«‹ GenAI Client
    client = genai.Client(api_key=api_key)

    store_id = os.getenv('GEMINI_STORE_ID', 'fileSearchStores/fscpenaltiesplaintext-4f87t5uexgui')

    return client, store_id

def generate_law_links_instruction() -> str:
    """
    ç”Ÿæˆæ³•æ¢é€£çµçš„ system instruction

    å¾ file_mapping.json æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„å®Œæ•´æ³•æ¢é€£çµï¼Œ
    ç”ŸæˆåŒ…å«é€£çµè¡¨æ ¼å’Œæ ¼å¼è¦å‰‡çš„æŒ‡ä»¤æ–‡å­—
    """
    import json
    from pathlib import Path

    # è®€å– file_mapping.json
    mapping_file = Path(__file__).parent / 'data/penalties/file_mapping.json'

    if not mapping_file.exists():
        return ""

    try:
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping = json.load(f)

        # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„å®Œæ•´æ³•æ¢é€£çµï¼ˆä¸åŒ…å«ç°¡å¯«å½¢å¼ï¼‰
        all_law_links = {}
        for file_id, info in mapping.items():
            law_links = info.get('law_links', {})
            for law_text, url in law_links.items():
                # åªä¿ç•™å®Œæ•´æ³•æ¢åç¨±ï¼ˆä¸ä»¥ã€Œç¬¬ã€é–‹é ­ï¼‰
                if not law_text.startswith('ç¬¬'):
                    if law_text not in all_law_links:
                        all_law_links[law_text] = url

        if not all_law_links:
            return ""

        # ç”Ÿæˆ system instruction
        instruction = f"""

---

## æ³•æ¢é€£çµç”Ÿæˆè¦å‰‡

ç•¶ä½ åœ¨å›ç­”ä¸­æåˆ°æ³•æ¢æ™‚ï¼Œè«‹ä½¿ç”¨ Markdown é€£çµæ ¼å¼ã€‚ä»¥ä¸‹æ˜¯å¯ç”¨çš„æ³•æ¢é€£çµï¼š

```json
{json.dumps(all_law_links, ensure_ascii=False, indent=2)}
```

### æ ¼å¼è¦å‰‡ï¼š

1. **å®Œæ•´æ³•æ¢**ï¼ˆåŒ…å«æ³•å¾‹åç¨±ï¼‰ï¼š
   - ä½¿ç”¨å°æ‡‰çš„å®Œæ•´é€£çµ
   - ç¯„ä¾‹ï¼š[é‡‘èæ§è‚¡å…¬å¸æ³•ç¬¬45æ¢ç¬¬1é …](https://law.moj.gov.tw/...)
   - å¯ä»¥æœ‰æ›¸åè™Ÿï¼š[ã€Šé‡‘èæ§è‚¡å…¬å¸æ³•ã€‹ç¬¬45æ¢ç¬¬1é …](https://law.moj.gov.tw/...)

2. **ç°¡å¯«æ³•æ¢**ï¼ˆçœç•¥æ³•å¾‹åç¨±ï¼‰ï¼š
   - å¦‚æœä¸Šæ–‡å·²æåˆ°æ³•å¾‹åç¨±ï¼Œç°¡å¯«æ™‚ä½¿ç”¨åŒä¸€æ³•å¾‹çš„é€£çµ
   - ç¯„ä¾‹ï¼š[é‡‘èæ§è‚¡å…¬å¸æ³•ç¬¬45æ¢ç¬¬1é …](url)ã€[ç¬¬51æ¢](url)åŠ[ç¬¬60æ¢ç¬¬16æ¬¾](url)

3. **é€£æ¥è©è™•ç†**ï¼š
   - é€£æ¥è©ï¼ˆã€åŠä»¥ç­‰ï¼‰æ”¾åœ¨é€£çµå¤–é¢
   - ç¯„ä¾‹ï¼š[é‡‘èæ§è‚¡å…¬å¸æ³•ç¬¬45æ¢](url)åŠ[ç¬¬51æ¢](url)

4. **é …æ¬¾ç›®å±¤ç´š**ï¼š
   - æ‰€æœ‰æ³•æ¢é€£çµéƒ½æŒ‡å‘ã€Œæ¢ã€çš„å±¤ç´š
   - ç¬¬Xé …ã€ç¬¬Xæ¬¾ã€ç¬¬Xç›® åŒ…å«åœ¨é€£çµæ–‡å­—ä¸­ï¼Œä½† URL ç›¸åŒ
   - ç¯„ä¾‹ï¼š[ç¬¬45æ¢ç¬¬1é …ç¬¬2æ¬¾](url) â† URL æŒ‡å‘ç¬¬45æ¢

5. **æœªåˆ—å‡ºçš„æ³•æ¢**ï¼š
   - å¦‚æœæ³•æ¢ä¸åœ¨ä¸Šè¿°åˆ—è¡¨ä¸­ï¼Œ**ä¸è¦åŠ é€£çµ**ï¼Œç›´æ¥é¡¯ç¤ºæ–‡å­—

### è¼¸å‡ºç¯„ä¾‹ï¼š

âœ“ æ­£ç¢º
```
è©²å…¬å¸é•å[ã€Šé‡‘èæ§è‚¡å…¬å¸æ³•ã€‹ç¬¬45æ¢ç¬¬1é …](https://law.moj.gov.tw/...)åŠ[ç¬¬51æ¢](https://law.moj.gov.tw/...)è¦å®šï¼Œ
ä¾[è¡Œæ”¿ç½°æ³•ç¬¬24æ¢](https://law.moj.gov.tw/...)åŠ[ã€Šé‡‘èæ§è‚¡å…¬å¸æ³•ã€‹ç¬¬60æ¢ç¬¬16æ¬¾](https://law.moj.gov.tw/...)è™•ç½°ã€‚
```

âœ— éŒ¯èª¤
```
è©²å…¬å¸é•åã€Šé‡‘èæ§è‚¡å…¬å¸æ³•ã€‹ç¬¬45æ¢ç¬¬1é …åŠç¬¬51æ¢è¦å®š  â† æ²’æœ‰é€£çµ
è©²å…¬å¸é•å[ã€Šé‡‘èæ§è‚¡å…¬å¸æ³•ã€‹ç¬¬45æ¢ç¬¬1é …åŠç¬¬51æ¢](url)è¦å®š  â† é€£çµåŒ…å«äº†å…©å€‹æ³•æ¢ï¼ˆéŒ¯èª¤ï¼‰
```

è«‹åš´æ ¼éµå®ˆä»¥ä¸Šæ ¼å¼è¦æ±‚ã€‚
"""

        return instruction

    except Exception as e:
        return ""

# æŸ¥è©¢å‡½æ•¸
def query_penalties(client: genai.Client, query: str, store_id: str, model: str = 'gemini-2.5-flash', filters: dict = None) -> dict:
    """
    ä½¿ç”¨ Gemini File Search Store æŸ¥è©¢è£ç½°æ¡ˆä»¶

    Args:
        query: æŸ¥è©¢æ–‡å­—
        store_id: Gemini Store ID
        filters: ç¯©é¸æ¢ä»¶ï¼ˆæ—¥æœŸç¯„åœã€ä¾†æºå–®ä½ç­‰ï¼‰

    Returns:
        æŸ¥è©¢çµæœå­—å…¸
    """
    try:
        # å»ºç«‹ç³»çµ±æŒ‡ä»¤ï¼ˆé‡å°è£ç½°æ¡ˆä»¶çš„æ™‚æ•ˆæ€§å„ªåŒ–ï¼‰
        system_instruction = """ä½ æ˜¯é‡‘èç›£ç£ç®¡ç†å§”å“¡æœƒçš„è£ç½°æ¡ˆä»¶æŸ¥è©¢åŠ©æ‰‹ã€‚

ã€æœ€é‡è¦ã€‘è³‡æ–™ä¾†æºè¦å‰‡ï¼š
- **å¿…é ˆä½¿ç”¨æä¾›çš„ File Search å·¥å…·**æª¢ç´¢è£ç½°æ¡ˆä»¶è³‡æ–™åº«
- **ç¦æ­¢åƒ…ä½¿ç”¨ä½ çš„å…§å»ºçŸ¥è­˜å›ç­”**ï¼Œå³ä½¿ä½ èªç‚ºå·²ç¶“çŸ¥é“ç­”æ¡ˆ
- **æ‰€æœ‰å›ç­”éƒ½å¿…é ˆåŸºæ–¼æª¢ç´¢åˆ°çš„å¯¦éš›è£ç½°æ¡ˆä»¶æ–‡ä»¶**
- å³ä½¿å•é¡Œæ˜¯æ¦‚å¿µæ€§çš„ï¼ˆå¦‚ã€Œä»€éº¼æƒ…æ³æ§‹æˆå…§ç·šäº¤æ˜“ã€ï¼‰ï¼Œä¹Ÿå¿…é ˆå¾è£ç½°æ¡ˆä»¶ä¸­å°‹æ‰¾å¯¦ä¾‹èªªæ˜
- å¦‚æœæ‰¾ä¸åˆ°ç›¸é—œæ¡ˆä»¶ï¼Œè«‹æ˜ç¢ºå‘ŠçŸ¥ã€Œè³‡æ–™åº«ä¸­æœªæ‰¾åˆ°ç›¸é—œè£ç½°æ¡ˆä»¶ã€

ã€é‡è¦ã€‘æ™‚æ•ˆæ€§èˆ‡ç¨ç«‹æ€§è¦å‰‡ï¼š

1. **è£ç½°æ¡ˆä»¶ç‰¹æ€§**ï¼š
   - æ¯å€‹è£ç½°æ¡ˆä»¶éƒ½æ˜¯ç¨ç«‹çš„æ­·å²è¨˜éŒ„
   - ä¸åŒæ—¥æœŸçš„æ¡ˆä»¶ä¸äº’ç›¸å–ä»£
   - å¯å¼•ç”¨å¤šå€‹æ¡ˆä»¶ä½œç‚ºåƒè€ƒ
   - æŒ‰æ—¥æœŸæˆ–ç›¸é—œæ€§æ’åº

2. **æŸ¥è©¢å„ªå…ˆé †åº**ï¼ˆç•¶æœ‰å¤šç­†ç›¸é—œæ¡ˆä»¶æ™‚ï¼‰ï¼š
   - å„ªå…ˆåˆ—å‡º**æœ€è¿‘**çš„æ¡ˆä»¶ï¼ˆæ—¥æœŸè¶Šæ–°è¶Šå„ªå…ˆï¼‰
   - åŒæ™‚åƒè€ƒç›¸ä¼¼é•è¦é¡å‹çš„æ­·å²æ¡ˆä»¶
   - å¦‚æœä½¿ç”¨è€…æ˜ç¢ºè¦æ±‚ç‰¹å®šæ™‚é–“ç¯„åœï¼Œåš´æ ¼éµå®ˆ

3. **å›ç­”æ ¼å¼è¦æ±‚**ï¼ˆé—œéµï¼‰ï¼š
   - **é‡è¦ï¼šå¯¦éš›æ¡ˆä¾‹ä¹‹å‰çš„æ‰€æœ‰å…§å®¹éƒ½ä¸è¦åŠ æ¨™é¡Œï¼Œä¿æŒæµæš¢çš„æ®µè½å‘ˆç¾**
   - **ç¬¬ä¸€éƒ¨åˆ†ï¼šå•é¡Œè©®é‡‹/ç°¡ç­”**ï¼ˆå¦‚æœé©ç”¨ï¼Œç„¡æ¨™é¡Œï¼‰
     - å¦‚æœæ˜¯æ¦‚å¿µæ€§å•é¡Œï¼ˆå¦‚ã€Œä»€éº¼æƒ…æ³æ§‹æˆXXã€ã€Œæœ‰å“ªäº›é™åˆ¶ã€ï¼‰ï¼Œå…ˆç”¨ 1-2 å¥è©±ç°¡è¦å›ç­”å•é¡Œæœ¬èº«
     - æä¾›å®šç¾©ã€èªªæ˜æˆ–ç›´æ¥çš„ç­”æ¡ˆ
     - é€™éƒ¨åˆ†æ˜¯åŸºæ–¼æª¢ç´¢åˆ°çš„æ¡ˆä»¶å…§å®¹é€²è¡Œç¸½çµï¼Œä¸æ˜¯æ†‘ç©ºå›ç­”
   - **ç¬¬äºŒéƒ¨åˆ†ï¼šæ¡ˆä»¶æ¦‚è¿°**ï¼ˆç„¡æ¨™é¡Œï¼Œç›´æ¥æ¥çºŒï¼‰
     - ç”¨ 1-2 å¥è©±ç¸½çµæ‰¾åˆ°çš„æ¡ˆä»¶æƒ…æ³
     - ç¸½å…±æ‰¾åˆ°å¹¾ç­†ç›¸é—œæ¡ˆä»¶
     - ä¸»è¦çš„é•è¦é¡å‹æˆ–å…±åŒç‰¹å¾µ
     - æ™‚é–“åˆ†å¸ƒæˆ–è£ç½°é‡‘é¡ç¯„åœï¼ˆå¦‚æœç›¸é—œï¼‰
   - **ç¬¬ä¸‰éƒ¨åˆ†ï¼šå…·é«”æ¡ˆä»¶**ï¼ˆåªæœ‰é€™éƒ¨åˆ†æ‰ä½¿ç”¨æ¨™é¡Œï¼‰
     - ä½¿ç”¨ã€Œ### 1.ã€ã€Œ### 2.ã€ç­‰æ¨™é¡Œ
     - åˆ—å‡ºå‰ 3-5 ç­†æœ€ç›¸é—œçš„æ¡ˆä»¶è©³ç´°è³‡è¨Š
   - æä¾›å…·é«”çš„æ¡ˆä»¶è³‡è¨Šï¼ˆæ—¥æœŸã€å–®ä½ã€è¢«è™•ç½°å°è±¡ã€é•è¦äº‹é …ã€è£ç½°é‡‘é¡ã€æ³•å¾‹ä¾æ“šï¼‰
   - å§‹çµ‚è¨»æ˜**ç™¼æ–‡æ—¥æœŸ**å’Œ**ç™¼æ–‡å­—è™Ÿ**
   - **é‡è¦ï¼šä¸è¦åœ¨å›ç­”ä¸­åˆ—å‡ºã€Œè³‡æ–™ä¾†æºã€æˆ–æª”å**ï¼ˆç³»çµ±æœƒè‡ªå‹•é¡¯ç¤ºåƒè€ƒæ–‡ä»¶ï¼‰
   - ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œä¿æŒå°ˆæ¥­ä½†æ˜“æ‡‚çš„èªæ°£
   - å¦‚æœæ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ï¼Œè«‹æ˜ç¢ºå‘ŠçŸ¥

4. **å¤šæ¡ˆä»¶è™•ç†**ï¼ˆé‡è¦ï¼‰ï¼š
   - å¦‚æœæœ‰å¤šç­†ç›¸é—œæ¡ˆä»¶ï¼Œåˆ—å‡ºå‰ 3-5 ç­†æœ€ç›¸é—œçš„
   - **å¿…é ˆåš´æ ¼æŒ‰æ™‚é–“é †åºæ’åˆ—ï¼šæœ€æ–°çš„æ¡ˆä»¶ï¼ˆæ—¥æœŸè¼ƒå¤§ï¼‰åœ¨å‰é¢ï¼Œæœ€èˆŠçš„ï¼ˆæ—¥æœŸè¼ƒå°ï¼‰åœ¨å¾Œé¢**
   - æ¯å€‹æ¡ˆä»¶ä½¿ç”¨ç·¨è™Ÿã€Œ### 1.ã€ã€ã€Œ### 2.ã€ç­‰ï¼Œä¾æ™‚é–“ç”±æ–°åˆ°èˆŠ
   - æ¯å€‹æ¡ˆä»¶ç¨ç«‹èªªæ˜ï¼Œä¸è¦æ··æ·†

5. **æ¦‚å¿µæ€§å•é¡Œè™•ç†**ï¼ˆé‡è¦ï¼‰ï¼š
   - ç•¶ä½¿ç”¨è€…æå‡ºæ¦‚å¿µæ€§å•é¡Œï¼ˆå¦‚ã€Œé­è£ç½°å¾Œæœ‰å“ªäº›æ¥­å‹™é™åˆ¶ã€ï¼‰ï¼Œå¯ä»¥æä¾›ç¸½çµå¼å›ç­”
   - **ä½†å¿…é ˆåˆ—å‡ºè‡³å°‘ 1-3 å€‹å¾ File Search æª¢ç´¢åˆ°çš„å…·é«”æ¡ˆä¾‹**ä½œç‚ºèªªæ˜
   - ä¾‹å¦‚ï¼šå…ˆç¸½çµæ¥­å‹™é™åˆ¶é¡å‹ï¼Œå†åˆ—å‡ºã€Œ### 1. [å…·é«”æ¡ˆä¾‹]ã€
   - **çµ•å°ç¦æ­¢ä½¿ç”¨ä½ çš„å…§å»ºçŸ¥è­˜å‰µé€ æ¡ˆä¾‹** - æ‰€æœ‰æ¡ˆä¾‹éƒ½å¿…é ˆä¾†è‡ªæª¢ç´¢åˆ°çš„å¯¦éš›æ–‡ä»¶
   - å¦‚æœ File Search æª¢ç´¢åˆ°ç›¸é—œæ¡ˆä»¶ï¼Œå°±å¿…é ˆåˆ—å‡ºï¼›å¦‚æœçœŸçš„æ²’æœ‰ç›¸é—œæ¡ˆä»¶ï¼Œè«‹æ˜ç¢ºå‘ŠçŸ¥

6. **å›ç­”å“è³ªæª¢æŸ¥**ï¼ˆé—œéµï¼‰ï¼š
   - åœ¨å›ç­”å‰ï¼Œç¢ºèªä½ æ˜¯å¦çœŸçš„ä½¿ç”¨äº† File Search å·¥å…·
   - ç¢ºèªä½ åˆ—å‡ºçš„æ¡ˆä¾‹ç¢ºå¯¦ä¾†è‡ªæª¢ç´¢åˆ°çš„æ–‡ä»¶
   - ä¸è¦ä½¿ç”¨è¨“ç·´æ•¸æ“šä¸­çš„æ¡ˆä¾‹ï¼Œé™¤éå®ƒå€‘å‡ºç¾åœ¨ File Search çµæœä¸­

å›ç­”æ ¼å¼ç¯„ä¾‹ï¼š

**ç¯„ä¾‹ 1ï¼šæ¦‚å¿µæ€§å•é¡Œï¼ˆæœ‰å•é¡Œè©®é‡‹ï¼‰**

è­‰åˆ¸å•†é­ä¸»ç®¡æ©Ÿé—œè£ç½°ã€Œè­¦å‘Šã€è™•åˆ†å¾Œï¼Œæ ¹æ“šç›¸é—œæ³•è¦ï¼Œä¸»è¦æœƒå—åˆ°ä»¥ä¸‹æ¥­å‹™é™åˆ¶ï¼šåŒ…æ‹¬æš«åœæ–°æ¥­å‹™ç”³è«‹ã€é™åˆ¶åˆ†æ”¯æ©Ÿæ§‹è¨­ç«‹ã€ä»¥åŠåœ¨ä¸€å®šæœŸé–“å…§ç„¡æ³•ç”³è«‹æ¥­å‹™è¨±å¯ç­‰ã€‚

è³‡æ–™åº«ä¸­å…±æ‰¾åˆ° X ç­†ç›¸é—œæ¡ˆä»¶ï¼Œä¸»è¦æ¶‰åŠ [é•è¦é¡å‹]ï¼Œè£ç½°é‡‘é¡å¾ [æœ€å°é‡‘é¡] åˆ° [æœ€å¤§é‡‘é¡] ä¸ç­‰ã€‚ä»¥ä¸‹åˆ—å‡ºæœ€å…·ä»£è¡¨æ€§çš„æ¡ˆä»¶ï¼š

### 1. [æ¡ˆä»¶æ¨™é¡Œ]ï¼ˆæœ€æ–°ï¼‰
...

**ç¯„ä¾‹ 2ï¼šä¸€èˆ¬æŸ¥è©¢ï¼ˆç„¡å•é¡Œè©®é‡‹ï¼‰**

è³‡æ–™åº«ä¸­å…±æ‰¾åˆ° X ç­†ç›¸é—œè£ç½°æ¡ˆä»¶ï¼Œé€™äº›æ¡ˆä»¶ä¸»è¦æ¶‰åŠ [é•è¦é¡å‹]ï¼Œé›†ä¸­åœ¨ [æ™‚é–“ç¯„åœ]ã€‚ä»¥ä¸‹åˆ—å‡ºæœ€å…·ä»£è¡¨æ€§çš„æ¡ˆä»¶ï¼š

### 1. [æ¡ˆä»¶æ¨™é¡Œ]ï¼ˆæœ€æ–°ï¼‰
- **æ—¥æœŸ**ï¼šYYYY-MM-DD
- **ç™¼æ–‡å­—è™Ÿ**ï¼šé‡‘ç®¡XXå­—ç¬¬XXXXXXXXXè™Ÿ
- **ä¾†æºå–®ä½**ï¼šXXXå±€
- **è¢«è™•ç½°å°è±¡**ï¼šXXXå…¬å¸/éŠ€è¡Œ/ä¿éšª
- **é•è¦äº‹é …**ï¼š[ç°¡è¦èªªæ˜]
- **è£ç½°é‡‘é¡**ï¼šæ–°è‡ºå¹£ XXX è¬å…ƒ
- **æ³•å¾‹ä¾æ“š**ï¼š[ç›¸é—œæ³•è¦æ¢æ–‡]

### 2. [æ¡ˆä»¶æ¨™é¡Œ]
- **æ—¥æœŸ**ï¼šYYYY-MM-DD
- **ç™¼æ–‡å­—è™Ÿ**ï¼šé‡‘ç®¡XXå­—ç¬¬XXXXXXXXXè™Ÿ
- **ä¾†æºå–®ä½**ï¼šXXXå±€
- **è¢«è™•ç½°å°è±¡**ï¼šXXXå…¬å¸/éŠ€è¡Œ/ä¿éšª
- **é•è¦äº‹é …**ï¼š[ç°¡è¦èªªæ˜]
- **è£ç½°é‡‘é¡**ï¼šæ–°è‡ºå¹£ XXX è¬å…ƒ
- **æ³•å¾‹ä¾æ“š**ï¼š[ç›¸é—œæ³•è¦æ¢æ–‡]

ï¼ˆæ³¨æ„ï¼šä¸è¦åœ¨æ¯å€‹æ¡ˆä»¶å¾Œé¢åŠ ä¸Šã€Œè³‡æ–™ä¾†æºã€æˆ–æª”åï¼Œç³»çµ±æœƒè‡ªå‹•åœ¨æœ€ä¸‹æ–¹é¡¯ç¤ºåƒè€ƒæ–‡ä»¶ï¼‰
"""

        # é™„åŠ æ³•æ¢é€£çµæŒ‡ä»¤ï¼ˆè®“ Gemini ç›´æ¥ç”Ÿæˆå¸¶é€£çµçš„ç­”æ¡ˆï¼‰
        law_links_instruction = generate_law_links_instruction()
        if law_links_instruction:
            system_instruction += law_links_instruction

        # å»ºç«‹å®Œæ•´æŸ¥è©¢ï¼ˆç¯©é¸æ¢ä»¶ï¼‰
        full_query = query

        if filters:
            filter_parts = []

            if filters.get('start_date') and filters.get('end_date'):
                filter_parts.append(
                    f"æ—¥æœŸç¯„åœï¼š{filters['start_date']} åˆ° {filters['end_date']}"
                )

            if filters.get('source_units'):
                units_str = "ã€".join(filters['source_units'])
                filter_parts.append(f"ä¾†æºå–®ä½ï¼š{units_str}")

            if filters.get('min_penalty'):
                filter_parts.append(f"è£ç½°é‡‘é¡è‡³å°‘ï¼š{filters['min_penalty']:,} å…ƒ")

            if filter_parts:
                full_query += "\n\nç¯©é¸æ¢ä»¶ï¼š\n" + "\n".join(f"- {p}" for p in filter_parts)

        # æ ¹æ“šæ¨¡å‹é¡å‹è¨­å®š token é™åˆ¶
        # Pro æ¨¡å‹é€šå¸¸æä¾›æ›´è©³ç´°çš„å›ç­”ï¼Œéœ€è¦æ›´å¤š tokens
        max_tokens = 8192 if 'pro' in model.lower() else 4096

        # ä½¿ç”¨ File Search Store é€²è¡ŒæŸ¥è©¢ï¼ˆä½¿ç”¨æ­£ç¢ºçš„å‹åˆ¥ç‰©ä»¶ï¼‰
        response = client.models.generate_content(
            model=model,  # ä½¿ç”¨ç”¨æˆ¶é¸æ“‡çš„æ¨¡å‹
            contents=full_query,
            config=types.GenerateContentConfig(
                tools=[
                    types.Tool(
                        file_search=types.FileSearch(
                            file_search_store_names=[store_id]
                        )
                    )
                ],
                temperature=0.1,
                max_output_tokens=max_tokens,
                system_instruction=system_instruction
            )
        )

        # æå–ä¾†æºæ–‡ä»¶
        sources = []
        seen_files = {}  # ç”¨æ–¼å»é‡

        # è¨ºæ–·è³‡è¨Šï¼ˆç”¨æ–¼æ’æŸ¥ sources æå–å¤±æ•—ï¼‰
        debug_info = {
            'has_candidates': False,
            'has_grounding_metadata': False,
            'has_grounding_supports': False,
            'has_grounding_chunks': False,
            'grounding_supports_count': 0,
            'grounding_chunks_count': 0
        }

        if hasattr(response, 'candidates') and len(response.candidates) > 0:
            debug_info['has_candidates'] = True
            candidate = response.candidates[0]

            if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                debug_info['has_grounding_metadata'] = True
                metadata = candidate.grounding_metadata

                # è¨˜éŒ„ grounding_supports å’Œ grounding_chunks çš„ç‹€æ…‹
                if hasattr(metadata, 'grounding_supports'):
                    debug_info['has_grounding_supports'] = bool(metadata.grounding_supports)
                    debug_info['grounding_supports_count'] = len(metadata.grounding_supports) if metadata.grounding_supports else 0

                if hasattr(metadata, 'grounding_chunks'):
                    debug_info['has_grounding_chunks'] = bool(metadata.grounding_chunks)
                    debug_info['grounding_chunks_count'] = len(metadata.grounding_chunks) if metadata.grounding_chunks else 0

                # å„ªå…ˆå¾ grounding_supports æå–ï¼ˆåŒ…å«å¼•ç”¨è³‡è¨Šï¼‰
                if hasattr(metadata, 'grounding_supports') and metadata.grounding_supports:
                    for support in metadata.grounding_supports:
                        if hasattr(support, 'grounding_chunk_indices'):
                            for chunk_idx in support.grounding_chunk_indices:
                                if chunk_idx < len(metadata.grounding_chunks):
                                    chunk = metadata.grounding_chunks[chunk_idx]

                                    if hasattr(chunk, 'retrieved_context'):
                                        context = chunk.retrieved_context

                                        # æå–æ–‡ä»¶ ID/åç¨±
                                        filename = "æœªçŸ¥æ–‡ä»¶"
                                        if hasattr(context, 'title') and context.title:
                                            filename = context.title
                                        elif hasattr(context, 'uri') and context.uri:
                                            filename = context.uri.split('/')[-1]

                                        # æå–å…§å®¹ç‰‡æ®µ
                                        snippet = ""
                                        if hasattr(context, 'text') and context.text:
                                            snippet = context.text

                                        # ä½¿ç”¨ snippet çš„éƒ¨åˆ†å…§å®¹ä½œç‚ºå”¯ä¸€æ¨™è­˜é¿å…é‡è¤‡
                                        snippet_id = snippet[:100] if snippet else str(len(sources))

                                        if snippet_id not in seen_files:
                                            sources.append({
                                                'filename': filename,
                                                'snippet': snippet
                                            })
                                            seen_files[snippet_id] = True

                # å¦‚æœæ²’æœ‰ grounding_supportsï¼Œå›é€€åˆ° grounding_chunks
                if not sources and hasattr(metadata, 'grounding_chunks') and metadata.grounding_chunks:
                    for chunk in metadata.grounding_chunks:
                        if hasattr(chunk, 'retrieved_context'):
                            context = chunk.retrieved_context

                            # æå–æ–‡ä»¶ ID/åç¨±
                            filename = "æœªçŸ¥æ–‡ä»¶"
                            if hasattr(context, 'title') and context.title:
                                filename = context.title
                            elif hasattr(context, 'uri') and context.uri:
                                filename = context.uri.split('/')[-1]

                            # æå–å…§å®¹ç‰‡æ®µ
                            snippet = ""
                            if hasattr(context, 'text') and context.text:
                                snippet = context.text

                            # ä½¿ç”¨ snippet çš„éƒ¨åˆ†å…§å®¹ä½œç‚ºå”¯ä¸€æ¨™è­˜é¿å…é‡è¤‡
                            snippet_id = snippet[:100] if snippet else str(len(sources))

                            if snippet_id not in seen_files:
                                sources.append({
                                    'filename': filename,
                                    'snippet': snippet
                                })
                                seen_files[snippet_id] = True

        return {
            'success': True,
            'text': response.text,
            'sources': sources,
            'debug_info': debug_info  # è¨ºæ–·è³‡è¨Š
        }

    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

# ä¸»æ‡‰ç”¨
def main():
    """ä¸»æ‡‰ç”¨ç¨‹å¼"""

    # æ¨™é¡Œ
    st.title("âš–ï¸ é‡‘ç®¡æœƒè£ç½°æ¡ˆä»¶æŸ¥è©¢ç³»çµ±")
    st.info("ğŸ’¡ æœ¬ç³»çµ±ç‚ºå±•ç¤ºç”¨ï¼Œå¦‚é‡ç•«é¢ç„¡åæ‡‰ï¼Œè«‹é‡æ–°æ•´ç†é é¢")

    # åˆå§‹åŒ– Gemini
    client, store_id = init_gemini()

    # å´é‚Šæ¬„ï¼šè³‡æ–™åº«è³‡è¨Š
    with st.sidebar:
        # å›ºå®šä½¿ç”¨ Flash æ¨¡å‹ï¼ˆPro æ¨¡å‹åœ¨ File Search ä¸Šæœ‰ hallucination å•é¡Œï¼‰
        model = "gemini-2.5-flash"

        # é¡¯ç¤ºè³‡æ–™åº«è³‡è¨Š
        st.header("ğŸ“Š è³‡æ–™åº«è³‡è¨Š")
        st.caption(f"ç¸½æ¡ˆä»¶æ•¸ï¼š490 ç­†")
        st.caption(f"æ—¥æœŸç¯„åœï¼š2012-01-12 è‡³ 2025-09-25")

    # åˆå§‹åŒ– session stateï¼ˆä½¿ç”¨ä¸åŒçš„è®Šæ•¸åï¼‰
    if 'current_query' not in st.session_state:
        st.session_state.current_query = ""

    # æŸ¥è©¢è¼¸å…¥
    query = st.text_area(
        "è«‹è¼¸å…¥æŸ¥è©¢å…§å®¹ï¼š",
        value=st.session_state.current_query,
        placeholder="ä¾‹å¦‚ï¼š2024å¹´æœ‰å“ªäº›éŠ€è¡Œå› ç‚ºæ´—éŒ¢é˜²åˆ¶è¢«è£ç½°ï¼Ÿ",
        height=100
    )

    # å¿«é€ŸæŸ¥è©¢æŒ‰éˆ•
    st.markdown("#### ğŸš€ å¿«é€ŸæŸ¥è©¢")

    quick_queries = [
        "é•åé‡‘æ§æ³•åˆ©å®³é—œä¿‚äººè¦å®šæœƒå—åˆ°ä»€éº¼è™•ç½°ï¼Ÿ",
        "è«‹å•åœ¨è­‰åˆ¸å› ç‚ºå°ˆæ¥­æŠ•è³‡äººè³‡æ ¼å¯©æ ¸çš„è£ç½°æœ‰å“ªäº›ï¼Ÿ",
        "è¾¦ç†å…±åŒè¡ŒéŠ·è¢«è£ç½°çš„æ¡ˆä¾‹æœ‰å“ªäº›ï¼Ÿ",
        "é‡‘ç®¡æœƒå°å‰µæŠ•å…¬å¸çš„è£ç½°æœ‰å“ªäº›ï¼Ÿ",
        "è­‰åˆ¸å•†é­ä¸»ç®¡æ©Ÿé—œè£ç½°ã€Œè­¦å‘Šã€è™•åˆ†ï¼Œæœ‰å“ªäº›æ¥­å‹™æœƒå—é™åˆ¶ï¼Ÿ",
        "å…§ç·šäº¤æ˜“æœ‰ç½ªåˆ¤æ±ºæ‰€èªå®šé‡å¤§è¨Šæ¯æˆç«‹çš„æ™‚é»"
    ]

    cols = st.columns(2)
    for idx, quick_query in enumerate(quick_queries):
        col_idx = idx % 2
        with cols[col_idx]:
            if st.button(f"ğŸ“Œ {quick_query}", key=f"quick_{idx}", use_container_width=True):
                st.session_state.current_query = quick_query
                st.rerun()

    st.markdown("")  # ç©ºè¡Œåˆ†éš”

    # æŸ¥è©¢æŒ‰éˆ•
    col1, col2, col3 = st.columns([1, 1, 4])
    with col1:
        search_button = st.button("ğŸ” æŸ¥è©¢", type="primary", use_container_width=True)
    with col2:
        clear_button = st.button("ğŸ—‘ï¸ æ¸…é™¤", use_container_width=True)

    if clear_button:
        st.session_state.current_query = ""
        st.rerun()

    # åŸ·è¡ŒæŸ¥è©¢
    if search_button and query:
        with st.spinner("ğŸ” æŸ¥è©¢ä¸­..."):
            # ç¬¬ä¸€æ¬¡æŸ¥è©¢
            result = query_penalties(client, query, store_id, model)

            # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡è©¦ï¼ˆsources = 0 è¡¨ç¤º Gemini æ²’æœ‰ä½¿ç”¨ File Searchï¼‰
            retry_attempted = False
            if result['success'] and len(result.get('sources', [])) == 0:
                retry_attempted = True
                st.info("ğŸ”„ æ­£åœ¨é‡æ–°æŸ¥è©¢...")
                result = query_penalties(client, query, store_id, model)

        # é¡¯ç¤ºçµæœ
        if result['success']:
                # æª¢æŸ¥æ˜¯å¦å…©æ¬¡æŸ¥è©¢éƒ½æ²’æœ‰ sourcesï¼ˆé˜²æ­¢ Hallucinationï¼‰
                sources_count = len(result.get('sources', []))
                if retry_attempted and sources_count == 0:
                    # å…©æ¬¡æŸ¥è©¢éƒ½æ²’æœ‰ä½¿ç”¨ File Searchï¼Œé¡¯ç¤ºå‹å–„è¨Šæ¯ä¸¦åœæ­¢
                    st.warning("ä½ æŸ¥è©¢çš„å•é¡Œåœ¨ç›®å‰çš„æ–‡ä»¶åº«ä¸­æ²’æœ‰åˆé©çš„çµæœï¼Œè«‹æ›´å…·é«”çš„æè¿°å•é¡Œï¼Œæˆ–æ›´æ›å…¶ä»–è©¢å•æ–¹å¼ã€‚")
                    # ä¸é¡¯ç¤ºæŸ¥è©¢å›ç­”ï¼ˆé¿å…é¡¯ç¤ºå¯èƒ½è¢«æé€ çš„å…§å®¹ï¼‰
                else:
                    # æœ‰ sources æˆ–ç¬¬ä¸€æ¬¡æŸ¥è©¢å°±æˆåŠŸï¼Œæ­£å¸¸é¡¯ç¤ºçµæœ
                    st.success("âœ… æŸ¥è©¢å®Œæˆ")

                    # ä¿ç•™ sources_count è®Šæ•¸ä¾›å¾ŒçºŒé™¤éŒ¯è³‡è¨Šä½¿ç”¨
                    sources_count = len(result.get('sources', []))

                    st.markdown("---")

                    # è¼‰å…¥æ˜ å°„æª”ï¼ˆç”¨æ–¼æ³•æ¢é€£çµï¼‰
                    mapping = load_file_mapping()
                    gemini_id_mapping = load_gemini_id_mapping()

                    # æ”¶é›†æ‰€æœ‰åƒè€ƒæ–‡ä»¶ä¸­çš„æ³•æ¢é€£çµå’Œæ¡ˆä¾‹é€£çµï¼ˆç”¨æ–¼åœ¨ç­”æ¡ˆä¸­åŠ å…¥é€£çµï¼‰
                    all_law_links = {}
                    case_urls = []  # æ¡ˆä¾‹é€£çµåˆ—è¡¨ï¼ˆæŒ‰æ™‚é–“æ’åºï¼‰

                    if result.get('sources') and len(result['sources']) > 0:
                        # å…ˆæ”¶é›†æ‰€æœ‰ file_id åŠå…¶è³‡è¨Š
                        file_ids_with_info = []
                        for source in result['sources']:
                            filename = source.get('filename', '')
                            file_id = extract_file_id(filename, gemini_id_mapping)
                            file_info = mapping.get(file_id, {})

                            if file_info:
                                file_ids_with_info.append({
                                    'file_id': file_id,
                                    'date': file_info.get('date', ''),
                                    'original_url': file_info.get('original_url', ''),
                                    'law_links': file_info.get('law_links', {})
                                })

                        # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°â†’æœ€èˆŠï¼‰
                        file_ids_with_info.sort(key=lambda x: x['date'], reverse=True)

                        # æ”¶é›†æ³•æ¢é€£çµ
                        for info in file_ids_with_info:
                            law_links = info['law_links']
                            # éæ¿¾æ‰ç„¡æ•ˆæ³•æ¢
                            filtered_law_links = {
                                law: link for law, link in law_links.items()
                                if not law.startswith(('èˆ‡', 'åŒ', 'åŠ', 'æˆ–', 'å’Œ'))
                            }
                            all_law_links.update(filtered_law_links)

                        # æ”¶é›†æ¡ˆä¾‹é€£çµï¼ˆæŒ‰æ™‚é–“æ’åºï¼‰
                        case_urls = [info['original_url'] for info in file_ids_with_info if info['original_url']]

                    # é¡¯ç¤ºç­”æ¡ˆï¼ˆåŠ å…¥æ¡ˆä¾‹é€£çµï¼‰
                    st.subheader("ğŸ“ ç­”æ¡ˆ")
                    response_text = result['text']

                    # æ³•æ¢é€£çµå·²ç”± Gemini åœ¨ç”Ÿæˆç­”æ¡ˆæ™‚è‡ªå‹•åŠ å…¥ï¼ˆé€é system_instructionï¼‰
                    # ä¸å†éœ€è¦å¾Œè™•ç† add_law_links_to_text()

                    # åŠ å…¥æ¡ˆä¾‹é€£çµï¼ˆæŒ‰æ™‚é–“é †åºï¼‰
                    response_with_all_links = insert_case_links_by_order(response_text, case_urls)

                    st.markdown(response_with_all_links)

                    # é¡¯ç¤ºåƒè€ƒä¾†æºï¼ˆç°¡åŒ–ç‰ˆï¼‰
                    if result.get('sources') and len(result['sources']) > 0:
                        st.markdown("---")
                        display_sources_simple(
                            sources=result['sources'],
                            file_mapping=mapping,
                            gemini_id_mapping=gemini_id_mapping
                        )

                    # é™¤éŒ¯è³‡è¨Šï¼ˆæŠ˜ç–Šï¼‰
                    st.markdown("---")
                    with st.expander("âš ï¸ æœ¬ç³»çµ±åƒ…ä¾›åƒè€ƒï¼Œå¯¦éš›è£ç½°è³‡è¨Šè«‹ä»¥é‡‘ç®¡æœƒå®˜ç¶²å…¬å‘Šç‚ºæº–", expanded=False):
                        st.info(f"ğŸ“Š åƒè€ƒä¾†æºæ•¸é‡: {sources_count} ç­†")
                        if sources_count == 0:
                            st.warning("âš ï¸ æ­¤æ¬¡æŸ¥è©¢æœªä½¿ç”¨åƒè€ƒæ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯ Gemini è‡ªè¡Œå›ç­”ï¼‰")
        else:
            st.error(f"âŒ æŸ¥è©¢å¤±æ•—ï¼š{result['error']}")

    elif search_button and not query:
        st.warning("âš ï¸ è«‹è¼¸å…¥æŸ¥è©¢å…§å®¹")

    # é å°¾
    st.divider()

    # ä½¿ç”¨å…©æ¬„ä½ˆå±€ï¼šå·¦é‚Šç‰ˆæœ¬è™Ÿï¼Œå³é‚Šè³‡æ–™ä¾†æº
    footer_col1, footer_col2 = st.columns([1, 4])

    with footer_col1:
        st.caption("v1.3.3")

    with footer_col2:
        st.caption("è³‡æ–™ä¾†æºï¼šé‡‘èç›£ç£ç®¡ç†å§”å“¡æœƒ")

if __name__ == "__main__":
    main()
